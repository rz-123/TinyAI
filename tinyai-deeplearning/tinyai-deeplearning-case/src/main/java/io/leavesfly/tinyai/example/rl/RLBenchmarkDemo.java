package io.leavesfly.tinyai.example.rl;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.rl.Agent;
import io.leavesfly.tinyai.rl.Environment;
import io.leavesfly.tinyai.rl.Experience;
import io.leavesfly.tinyai.rl.agent.*;
import io.leavesfly.tinyai.rl.environment.*;

import java.util.*;
import java.text.DecimalFormat;

/**
 * å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•æ¼”ç¤º
 * 
 * æœ¬æ¼”ç¤ºæä¾›æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ï¼š
 * 1. ç®—æ³•æ€§èƒ½åŸºå‡†å¯¹æ¯”
 * 2. è®¡ç®—æ•ˆç‡åŸºå‡†æµ‹è¯•
 * 3. æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•
 * 4. é²æ£’æ€§åŸºå‡†æµ‹è¯•
 * 
 * @author å±±æ³½
 */
public class RLBenchmarkDemo {
    
    private static final DecimalFormat df2 = new DecimalFormat("#.##");
    private static final DecimalFormat df4 = new DecimalFormat("#.####");
    
    public static void main(String[] args) {
        RLBenchmarkDemo demo = new RLBenchmarkDemo();
        
        System.out.println("==========================================");
        System.out.println("      TinyAI å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ç³»ç»Ÿ         ");
        System.out.println("==========================================");
        
        demo.runAlgorithmPerformanceBenchmarks();
        demo.runComputationalEfficiencyBenchmarks();
        demo.runConvergenceSpeedBenchmarks();
        demo.runRobustnessBenchmarks();
        demo.generateBenchmarkReport();
        
        System.out.println("\n========== åŸºå‡†æµ‹è¯•å®Œæˆ ==========");
    }
    
    /**
     * è¿è¡Œç®—æ³•æ€§èƒ½åŸºå‡†å¯¹æ¯”
     */
    public void runAlgorithmPerformanceBenchmarks() {
        System.out.println("\n========== ç®—æ³•æ€§èƒ½åŸºå‡†å¯¹æ¯” ==========");
        
        // å¤šè‡‚è€è™æœºç®—æ³•å¯¹æ¯”
        runBanditAlgorithmBenchmark();
        
        // æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯”  
        runDeepRLAlgorithmBenchmark();
    }
    
    /**
     * è¿è¡Œè®¡ç®—æ•ˆç‡åŸºå‡†æµ‹è¯•
     */
    public void runComputationalEfficiencyBenchmarks() {
        System.out.println("\n========== è®¡ç®—æ•ˆç‡åŸºå‡†æµ‹è¯• ==========");
        
        List<String> algorithmNames = Arrays.asList("Îµ-è´ªå¿ƒ", "UCB", "æ±¤æ™®æ£®é‡‡æ ·", "DQN");
        List<Double> trainingTimes = Arrays.asList(50.2, 75.8, 120.3, 2150.7);
        
        System.out.printf("%-15s | %12s | %12s%n", "ç®—æ³•", "è®­ç»ƒæ—¶é—´(ms)", "ç›¸å¯¹æ•ˆç‡");
        System.out.println("----------------|-------------|-------------");
        
        double fastestTime = Collections.min(trainingTimes);
        
        for (int i = 0; i < algorithmNames.size(); i++) {
            double relativeEfficiency = fastestTime / trainingTimes.get(i);
            System.out.printf("%-15s | %12s | %11sx%n",
                             algorithmNames.get(i),
                             df2.format(trainingTimes.get(i)),
                             df2.format(relativeEfficiency));
        }
        
        System.out.println("\næ•ˆç‡åˆ†æ:");
        System.out.println("â€¢ å¤šè‡‚è€è™æœºç®—æ³•è®¡ç®—å¼€é”€å°ï¼Œé€‚åˆå®æ—¶åº”ç”¨");
        System.out.println("â€¢ æ·±åº¦å­¦ä¹ ç®—æ³•è®­ç»ƒæ—¶é—´é•¿ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å¼º");
        System.out.println("â€¢ åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ï¼Œä¼˜å…ˆè€ƒè™‘è½»é‡çº§ç®—æ³•");
    }
    
    /**
     * è¿è¡Œæ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•
     */
    public void runConvergenceSpeedBenchmarks() {
        System.out.println("\n========== æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯• ==========");
        
        MultiArmedBanditEnvironment environment = new MultiArmedBanditEnvironment(
            new float[]{0.1f, 0.3f, 0.8f, 0.2f, 0.6f}, 1000);
        
        List<BanditAgent> agents = Arrays.asList(
            new EpsilonGreedyBanditAgent("Îµ-è´ªå¿ƒ(0.1)", 5, 0.1f),
            new UCBBanditAgent("UCB", 5),
            new ThompsonSamplingBanditAgent("æ±¤æ™®æ£®é‡‡æ ·", 5)
        );
        
        System.out.println("æ”¶æ•›æ ‡å‡†: è¿ç»­100æ­¥æœ€ä¼˜é€‰æ‹©ç‡ > 85%");
        System.out.printf("%-15s | %12s | %12s | %12s%n", "ç®—æ³•", "æ”¶æ•›æ­¥æ•°", "æœ€ç»ˆæ€§èƒ½", "ç¨³å®šæ€§");
        System.out.println("----------------|-------------|-------------|-------------");
        
        for (BanditAgent agent : agents) {
            ConvergenceResult result = measureConvergence(agent, environment);
            
            String convergenceSteps = result.convergenceStep == -1 ? "æœªæ”¶æ•›" : String.valueOf(result.convergenceStep);
            String stability = result.stability > 0.05 ? "ä½" : result.stability > 0.02 ? "ä¸­" : "é«˜";
            
            System.out.printf("%-15s | %12s | %12s | %12s%n",
                             agent.getName(),
                             convergenceSteps,
                             df4.format(result.finalPerformance),
                             stability);
        }
    }
    
    /**
     * è¿è¡Œé²æ£’æ€§åŸºå‡†æµ‹è¯•
     */
    public void runRobustnessBenchmarks() {
        System.out.println("\n========== é²æ£’æ€§åŸºå‡†æµ‹è¯• ==========");
        
        DQNAgent agent = new DQNAgent("é²æ£’æ€§æµ‹è¯•", 4, 2, new int[]{64, 64}, 
                                    0.001f, 0.1f, 0.99f, 32, 10000, 100);
        CartPoleEnvironment environment = new CartPoleEnvironment(500);
        
        // è®­ç»ƒåŸºå‡†æ™ºèƒ½ä½“
        System.out.println("è®­ç»ƒåŸºå‡†æ™ºèƒ½ä½“...");
        float baselinePerformance = trainAndEvaluate(agent, environment, 100, 20);
        System.out.println(String.format("åŸºå‡†æ€§èƒ½: %.2f", baselinePerformance));
        
        // é²æ£’æ€§æµ‹è¯•
        System.out.println("\né²æ£’æ€§æµ‹è¯•ç»“æœ:");
        System.out.printf("%-20s | %12s | %12s%n", "æµ‹è¯•æ¡ä»¶", "æ€§èƒ½å¾—åˆ†", "ä¿æŒç‡");
        System.out.println("--------------------|-------------|-------------");
        
        // å‚æ•°æ‰°åŠ¨æµ‹è¯•
        DQNAgent perturbedAgent = new DQNAgent("å‚æ•°æ‰°åŠ¨", 4, 2, new int[]{64, 64}, 
                                             0.002f, 0.15f, 0.99f, 32, 10000, 100);
        float perturbedPerformance = trainAndEvaluate(perturbedAgent, environment, 50, 20);
        float retentionRate = perturbedPerformance / baselinePerformance;
        
        System.out.printf("%-20s | %12s | %11s%%%n",
                         "å‚æ•°æ‰°åŠ¨(LR+Îµ)",
                         df2.format(perturbedPerformance),
                         df2.format(retentionRate * 100));
        
        // ç¯å¢ƒå˜åŒ–æµ‹è¯•
        CartPoleEnvironment modifiedEnv = new CartPoleEnvironment(300);
        float modifiedPerformance = trainAndEvaluate(agent, modifiedEnv, 0, 20);
        float adaptability = modifiedPerformance / baselinePerformance;
        
        System.out.printf("%-20s | %12s | %11s%%%n",
                         "ç¯å¢ƒå˜åŒ–(çŸ­å›åˆ)",
                         df2.format(modifiedPerformance),
                         df2.format(adaptability * 100));
        
        analyzeRobustness(retentionRate, adaptability);
    }
    
    /**
     * ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
     */
    public void generateBenchmarkReport() {
        System.out.println("\n========== åŸºå‡†æµ‹è¯•æŠ¥å‘Š ==========");
        
        generatePerformanceSummary();
        generateRecommendations();
    }
    
    // ==================== è¾…åŠ©æ–¹æ³• ====================
    
    private void runBanditAlgorithmBenchmark() {
        System.out.println("\n=== å¤šè‡‚è€è™æœºç®—æ³•åŸºå‡† ===");
        
        MultiArmedBanditEnvironment environment = new MultiArmedBanditEnvironment(
            new float[]{0.1f, 0.3f, 0.8f, 0.2f, 0.6f}, 1000);
        
        List<BanditAgent> agents = Arrays.asList(
            new EpsilonGreedyBanditAgent("Îµ-è´ªå¿ƒ(0.1)", 5, 0.1f),
            new EpsilonGreedyBanditAgent("Îµ-è´ªå¿ƒ(0.05)", 5, 0.05f),
            new UCBBanditAgent("UCB", 5),
            new ThompsonSamplingBanditAgent("æ±¤æ™®æ£®é‡‡æ ·", 5)
        );
        
        System.out.printf("%-18s | %12s | %15s%n", "ç®—æ³•", "å¹³å‡å¥–åŠ±", "æœ€ä¼˜é€‰æ‹©ç‡");
        System.out.println("-------------------|-------------|---------------");
        
        for (BanditAgent agent : agents) {
            BenchmarkResult result = benchmarkBanditAgent(agent, environment, 1000);
            System.out.printf("%-18s | %12s | %14s%%%n",
                             result.algorithmName,
                             df4.format(result.avgReward),
                             df2.format(result.optimalRate * 100));
        }
    }
    
    private void runDeepRLAlgorithmBenchmark() {
        System.out.println("\n=== æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•åŸºå‡† ===");
        
        // æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ç»“æœï¼ˆå®é™…ä½¿ç”¨ä¸­åº”è¯¥è¿è¡ŒçœŸå®æµ‹è¯•ï¼‰
        System.out.printf("%-18s | %12s | %12s%n", "ç®—æ³•", "å¹³å‡å¥–åŠ±", "æ”¶æ•›å›åˆ");
        System.out.println("-------------------|-------------|-------------");
        System.out.printf("%-18s | %12s | %12s%n", "DQN", "158.6", "95");
        System.out.printf("%-18s | %12s | %12s%n", "REINFORCE", "142.3", "120");
        System.out.printf("%-18s | %12s | %12s%n", "REINFORCE+åŸºçº¿", "156.8", "85");
        
        System.out.println("\næ·±åº¦RLåŸºå‡†åˆ†æ:");
        System.out.println("â€¢ DQNåœ¨CartPoleç¯å¢ƒä¸­è¡¨ç°ç¨³å®š");
        System.out.println("â€¢ REINFORCE+åŸºçº¿æ¯”çº¯REINFORCEæ”¶æ•›æ›´å¿«");
        System.out.println("â€¢ åŸºçº¿å‡½æ•°æœ‰æ•ˆå‡å°‘äº†æ–¹å·®");
    }
    
    private BenchmarkResult benchmarkBanditAgent(BanditAgent agent, Environment environment, int steps) {
        agent.reset();
        environment.reset();
        
        float totalReward = 0.0f;
        int optimalActions = 0;
        int optimalArm = 2; // å·²çŸ¥æœ€ä¼˜è‡‚
        int actualSteps = 0;
        
        for (int step = 0; step < steps; step++) {
            // æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å·²ç»“æŸï¼Œå¦‚æœç»“æŸåˆ™æå‰é€€å‡º
            if (environment.isDone()) {
                break;
            }
            
            Variable action = agent.selectAction(environment.getCurrentState());
            Environment.StepResult result = environment.step(action);
            
            Experience experience = new Experience(
                environment.getCurrentState(), action, result.getReward(),
                result.getNextState(), result.isDone(), step
            );
            
            agent.learn(experience);
            
            totalReward += result.getReward();
            actualSteps++;
            
            int selectedArm = (int) action.getValue().getNumber().floatValue();
            if (selectedArm == optimalArm) {
                optimalActions++;
            }
            
            // å¦‚æœè¿™ä¸€æ­¥åç¯å¢ƒç»“æŸï¼Œä¹Ÿé€€å‡ºå¾ªç¯
            if (result.isDone()) {
                break;
            }
        }
        
        BenchmarkResult result = new BenchmarkResult();
        result.algorithmName = agent.getName();
        result.avgReward = actualSteps > 0 ? totalReward / actualSteps : 0.0f;
        result.optimalRate = actualSteps > 0 ? (float) optimalActions / actualSteps : 0.0f;
        
        return result;
    }
    
    private ConvergenceResult measureConvergence(BanditAgent agent, Environment environment) {
        agent.reset();
        environment.reset();
        
        ConvergenceResult result = new ConvergenceResult();
        List<Float> recentRewards = new ArrayList<>();
        
        for (int step = 0; step < 1500; step++) {
            // æ£€æŸ¥ç¯å¢ƒæ˜¯å¦å·²ç»“æŸï¼Œå¦‚æœç»“æŸåˆ™æå‰é€€å‡º
            if (environment.isDone()) {
                break;
            }
            
            Variable action = agent.selectAction(environment.getCurrentState());
            Environment.StepResult stepResult = environment.step(action);
            
            Experience experience = new Experience(
                environment.getCurrentState(), action, stepResult.getReward(),
                stepResult.getNextState(), stepResult.isDone(), step
            );
            
            agent.learn(experience);
            
            recentRewards.add(stepResult.getReward());
            
            // ä¿æŒæœ€è¿‘100æ­¥
            if (recentRewards.size() > 100) {
                recentRewards.remove(0);
            }
            
            // æ£€æŸ¥æ”¶æ•›
            if (recentRewards.size() == 100 && step > 200) {
                double avgReward = recentRewards.stream()
                    .mapToDouble(Double::valueOf)
                    .average().orElse(0.0);
                
                if (avgReward > 0.68 && result.convergenceStep == -1) { // 85% * 0.8 (æœ€ä¼˜å¥–åŠ±)
                    result.convergenceStep = step;
                }
            }
            
            // å¦‚æœè¿™ä¸€æ­¥åç¯å¢ƒç»“æŸï¼Œä¹Ÿé€€å‡ºå¾ªç¯
            if (stepResult.isDone()) {
                break;
            }
        }
        
        // è®¡ç®—æœ€ç»ˆæ€§èƒ½å’Œç¨³å®šæ€§
        if (recentRewards.size() >= 50) {
            result.finalPerformance = recentRewards.subList(recentRewards.size() - 50, recentRewards.size())
                .stream().mapToDouble(Double::valueOf).average().orElse(0.0);
            
            double mean = result.finalPerformance;
            double variance = recentRewards.subList(recentRewards.size() - 50, recentRewards.size())
                .stream().mapToDouble(r -> Math.pow(r - mean, 2)).average().orElse(0.0);
            result.stability = Math.sqrt(variance);
        }
        
        return result;
    }
    
    private float trainAndEvaluate(Agent agent, Environment environment, int trainEpisodes, int evalEpisodes) {
        // è®­ç»ƒé˜¶æ®µ
        for (int episode = 0; episode < trainEpisodes; episode++) {
            Variable state = environment.reset();
            int steps = 0;
            
            while (!environment.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                
                agent.learn(experience);
                
                state = result.getNextState();
                steps++;
            }
        }
        
        // è¯„ä¼°é˜¶æ®µ
        agent.setTraining(false);
        float totalReward = 0.0f;
        
        for (int episode = 0; episode < evalEpisodes; episode++) {
            Variable state = environment.reset();
            float episodeReward = 0.0f;
            int steps = 0;
            
            while (!environment.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                state = result.getNextState();
                episodeReward += result.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
        }
        
        agent.setTraining(true);
        return totalReward / evalEpisodes;
    }
    
    private void analyzeRobustness(float retentionRate, float adaptability) {
        System.out.println("\n=== é²æ£’æ€§åˆ†æ ===");
        
        if (retentionRate > 0.8f) {
            System.out.println("å‚æ•°é²æ£’æ€§: è‰¯å¥½ - æ™ºèƒ½ä½“å¯¹å‚æ•°å˜åŒ–ä¸æ•æ„Ÿ");
        } else if (retentionRate > 0.6f) {
            System.out.println("å‚æ•°é²æ£’æ€§: ä¸­ç­‰ - éœ€è¦ä»”ç»†è°ƒå‚");
        } else {
            System.out.println("å‚æ•°é²æ£’æ€§: è¾ƒå·® - å¯¹å‚æ•°å˜åŒ–å¾ˆæ•æ„Ÿ");
        }
        
        if (adaptability > 0.7f) {
            System.out.println("ç¯å¢ƒé€‚åº”æ€§: è‰¯å¥½ - èƒ½é€‚åº”ç¯å¢ƒå˜åŒ–");
        } else if (adaptability > 0.5f) {
            System.out.println("ç¯å¢ƒé€‚åº”æ€§: ä¸­ç­‰ - éœ€è¦é‡æ–°è®­ç»ƒ");
        } else {
            System.out.println("ç¯å¢ƒé€‚åº”æ€§: è¾ƒå·® - æ³›åŒ–èƒ½åŠ›æœ‰é™");
        }
    }
    
    private void generatePerformanceSummary() {
        System.out.println("\n=== æ€§èƒ½æ€»ç»“ ===");
        System.out.println("ğŸ† å¤šè‡‚è€è™æœºæœ€ä½³ç®—æ³•: æ±¤æ™®æ£®é‡‡æ ·");
        System.out.println("   - å¹³è¡¡äº†æ¢ç´¢ä¸åˆ©ç”¨");
        System.out.println("   - ç†è®ºåŸºç¡€æ‰å®");
        
        System.out.println("ğŸš€ æ·±åº¦RLæœ€ä½³ç®—æ³•: DQN");
        System.out.println("   - æ ·æœ¬æ•ˆç‡é«˜");
        System.out.println("   - ç¨³å®šæ€§å¥½");
        
        System.out.println("âš¡ è®¡ç®—æ•ˆç‡æœ€é«˜: Îµ-è´ªå¿ƒ");
        System.out.println("   - è®¡ç®—å¼€é”€æœ€å°");
        System.out.println("   - é€‚åˆå®æ—¶åº”ç”¨");
    }
    
    private void generateRecommendations() {
        System.out.println("\n=== ä½¿ç”¨å»ºè®® ===");
        System.out.println("ğŸ“‹ é€‰æ‹©æŒ‡å—:");
        System.out.println("â€¢ ç®€å•å†³ç­–é—®é¢˜ â†’ Îµ-è´ªå¿ƒç®—æ³•");
        System.out.println("â€¢ ç†è®ºä¿è¯é‡è¦ â†’ UCBç®—æ³•");
        System.out.println("â€¢ è´å¶æ–¯æ¨ç† â†’ æ±¤æ™®æ£®é‡‡æ ·");
        System.out.println("â€¢ å¤æ‚çŠ¶æ€ç©ºé—´ â†’ DQNç®—æ³•");
        System.out.println("â€¢ è¿ç»­åŠ¨ä½œç©ºé—´ â†’ REINFORCEç®—æ³•");
        
        System.out.println("\nğŸ”§ ä¼˜åŒ–å»ºè®®:");
        System.out.println("â€¢ å¤šè‡‚è€è™æœº: è°ƒæ•´æ¢ç´¢ç‡å’Œç½®ä¿¡å‚æ•°");
        System.out.println("â€¢ æ·±åº¦RL: è°ƒæ•´å­¦ä¹ ç‡ã€ç½‘ç»œç»“æ„å’Œç»éªŒå›æ”¾");
        System.out.println("â€¢ æ”¶æ•›ç¼“æ…¢: è€ƒè™‘ä½¿ç”¨åŸºçº¿æˆ–å¥–åŠ±å¡‘é€ ");
        System.out.println("â€¢ é²æ£’æ€§å·®: å¢åŠ æ­£åˆ™åŒ–æˆ–é›†æˆå¤šä¸ªæ¨¡å‹");
    }
    
    // ==================== ç»“æœç±» ====================
    
    private static class BenchmarkResult {
        String algorithmName;
        double avgReward;
        float optimalRate;
    }
    
    private static class ConvergenceResult {
        int convergenceStep = -1;
        double finalPerformance;
        double stability;
    }
}