package io.leavesfly.tinyai.ml.parallel;

import io.leavesfly.tinyai.ml.Model;
import io.leavesfly.tinyai.ml.exception.TrainingException;
import io.leavesfly.tinyai.ml.parameter.ParameterOperator;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.nnet.Parameter;
import io.leavesfly.tinyai.nnet.v2.core.Module;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.Map;

/**
 * 并行训练工具类
 * <p>
 * 提供模型深拷贝、梯度同步等并行训练所需的工具方法
 */
public class ParallelTrainingUtils {

    /**
     * 创建模型的深拷贝，用于多线程训练
     * 每个线程需要独立的模型实例来避免参数冲突
     * <p>
     * 优化策略：
     * 1. 优先使用基于参数复制的方式（更快）
     * 2. 如果失败，回退到序列化方式
     *
     * @param originalModel 原始模型
     * @return 深拷贝的模型实例
     * @throws TrainingException 如果拷贝失败
     */
    public static Model deepCopyModel(Model originalModel) {
        if (originalModel == null) {
            throw new IllegalArgumentException("原始模型不能为空");
        }
        
        try {
            // 方案1：基于参数复制（更快，避免序列化开销）
            return deepCopyModelByParameters(originalModel);
        } catch (Exception e) {
            // 方案2：回退到序列化方式（兼容性更好）
            try {
                return deepCopyModelBySerialization(originalModel);
            } catch (Exception e2) {
                throw new TrainingException("模型深拷贝失败: " + e2.getMessage(), e2);
            }
        }
    }
    
    /**
     * 基于参数复制的模型深拷贝（优化方案）
     * 
     * @param originalModel 原始模型
     * @return 深拷贝的模型
     */
    private static Model deepCopyModelByParameters(Model originalModel) {
        // 获取原始模型的Module
        Module originalModule = originalModel.getModule();
        
        // 创建新模型（使用相同的Module结构，但参数会被复制）
        Model copiedModel = new Model(originalModel.getName() + "_copy", originalModule);
        
        // 复制所有参数
        Map<String, Parameter> originalParams = originalModel.getAllParams();
        Map<String, Parameter> copiedParams = copiedModel.getAllParams();
        
        for (Map.Entry<String, Parameter> entry : originalParams.entrySet()) {
            String paramName = entry.getKey();
            Parameter originalParam = entry.getValue();
            
            if (copiedParams.containsKey(paramName)) {
                Parameter copiedParam = copiedParams.get(paramName);
                try {
                    // 使用统一的参数复制接口
                    ParameterOperator.copyParameter(originalParam, copiedParam);
                } catch (Exception e) {
                    throw new TrainingException("复制参数 " + paramName + " 失败: " + e.getMessage(), e);
                }
            }
        }
        
        // 复制模型信息
        if (originalModel.getModelInfo() != null) {
            copiedModel.setModelInfo(originalModel.getModelInfo());
        }
        
        return copiedModel;
    }
    
    /**
     * 基于序列化的模型深拷贝（备用方案）
     * 
     * @param originalModel 原始模型
     * @return 深拷贝的模型
     */
    private static Model deepCopyModelBySerialization(Model originalModel) throws Exception {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ObjectOutputStream oos = new ObjectOutputStream(baos);
        oos.writeObject(originalModel);
        oos.close();

        ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
        ObjectInputStream ois = new ObjectInputStream(bais);
        Model copiedModel = (Model) ois.readObject();
        ois.close();

        return copiedModel;
    }

    /**
     * 将聚合后的梯度应用到主模型的参数上
     *
     * @param model               主模型
     * @param aggregatedGradients 聚合后的梯度
     */
    public static void applyAggregatedGradients(Model model, Map<String, NdArray> aggregatedGradients) {
        Map<String, Parameter> modelParams = model.getAllParams();

        for (Map.Entry<String, Parameter> entry : modelParams.entrySet()) {
            String paramName = entry.getKey();
            Parameter parameter = entry.getValue();

            // 获取对应的聚合梯度
            NdArray aggregatedGrad = aggregatedGradients.get(paramName);
            if (aggregatedGrad != null) {
                // 将聚合梯度设置到参数上
                parameter.setGrad(aggregatedGrad);
            }
        }
    }

    /**
     * 计算并行训练的推荐线程数
     * 基于CPU核心数和数据批次数量
     *
     * @param batchCount 数据批次总数
     * @return 推荐的线程数
     */
    public static int getRecommendedThreadCount(int batchCount) {
        int availableCores = Runtime.getRuntime().availableProcessors();

        // 线程数不应超过可用核心数的75%，也不应超过批次数量
        int maxThreads = Math.max(1, (int) (availableCores * 0.75));
        return Math.min(maxThreads, batchCount);
    }

    /**
     * 检查模型是否支持并行训练
     * 主要检查模型是否可序列化
     *
     * @param model 要检查的模型
     * @return true 如果支持并行训练
     */
    public static boolean isModelParallelizable(Model model) {
        try {
            // 尝试序列化测试
            ByteArrayOutputStream baos = new ByteArrayOutputStream();
            ObjectOutputStream oos = new ObjectOutputStream(baos);
            oos.writeObject(model);
            oos.close();
            return true;
        } catch (Exception e) {
            return false;
        }
    }

    /**
     * 格式化并行训练的统计信息
     *
     * @param threadCount       线程数
     * @param successfulBatches 成功处理的批次数
     * @param totalBatches      总批次数
     * @param averageLoss       平均损失
     * @param processingTimeMs  处理时间（毫秒）
     * @return 格式化的统计信息字符串
     */
    public static String formatParallelStats(int threadCount, int successfulBatches,
                                             int totalBatches, float averageLoss, long processingTimeMs) {
        return String.format(
                "并行训练统计 [线程数: %d, 成功批次: %d/%d, 平均损失: %.6f, 处理时间: %dms]",
                threadCount, successfulBatches, totalBatches, averageLoss, processingTimeMs
        );
    }
}