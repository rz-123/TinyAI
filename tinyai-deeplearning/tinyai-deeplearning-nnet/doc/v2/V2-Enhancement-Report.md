# TinyAI Neural Network V2 å¢å¼ºæŠ¥å‘Š

## æ¦‚è¿°

æœ¬æ¬¡æ›´æ–°å‚è€ƒ PyTorch è®¾è®¡ï¼Œå¯¹ `tinyai-deeplearning-nnet` çš„ V2 æ¨¡å—è¿›è¡Œäº†å…¨é¢å®Œå–„ï¼Œæ–°å¢äº†å¤§é‡æ¿€æ´»å‡½æ•°ã€å½’ä¸€åŒ–å±‚ã€Transformer ç»„ä»¶ä»¥åŠå¢å¼ºäº†æ ¸å¿ƒ Module ç±»çš„åŠŸèƒ½ã€‚

## æ›´æ–°å†…å®¹æ±‡æ€»

### ä¸€ã€æ¿€æ´»å‡½æ•°æ‰©å±• âœ…

| æ¿€æ´»å‡½æ•° | æ–‡ä»¶ | å…¬å¼ | åº”ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **GELU** | `layer/activation/GELU.java` | x * Î¦(x) â‰ˆ 0.5x(1+tanh(âˆš(2/Ï€)(x+0.044715xÂ³))) | GPT, BERT, ViT |
| **SiLU** | `layer/activation/SiLU.java` | x * sigmoid(x) | EfficientNet, YOLOv5 |
| **LeakyReLU** | `layer/activation/LeakyReLU.java` | max(Î±x, x), Î±=0.01 | è§£å†³ç¥ç»å…ƒæ­»äº¡ |
| **ELU** | `layer/activation/ELU.java` | x if xâ‰¥0, Î±(eË£-1) otherwise | è´Ÿå€¼é¥±å’Œï¼ŒåŠ é€Ÿå­¦ä¹  |
| **LogSoftmax** | `layer/activation/LogSoftmax.java` | log(softmax(x)) | é…åˆNLLLossä½¿ç”¨ |

**åº•å±‚å®ç°ï¼ˆfuncæ¨¡å—ï¼‰**ï¼š
- `func/math/SiLU.java` - åº•å±‚ SiLU Function
- `func/math/LeakyReLU.java` - åº•å±‚ LeakyReLU Function
- `func/math/ELU.java` - åº•å±‚ ELU Function
- `func/math/LogSoftmax.java` - åº•å±‚ LogSoftmax Function

**Variable æ‰©å±•æ–¹æ³•**ï¼š
- `gelu()` - GELUæ¿€æ´»
- `silu()` - SiLUæ¿€æ´»
- `leakyRelu(float negativeSlope)` - LeakyReLUæ¿€æ´»
- `elu(float alpha)` - ELUæ¿€æ´»
- `logSoftmax(int axis)` - LogSoftmaxæ¿€æ´»

---

### äºŒã€å½’ä¸€åŒ–å±‚å¢å¼º âœ…

| å½’ä¸€åŒ–å±‚ | æ–‡ä»¶ | å…¬å¼ | åº”ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **RMSNorm** | `layer/norm/RMSNorm.java` | y = x/RMS(x) * weight | LLaMA, DeepSeekç­‰LLM |

**RMSNorm ç‰¹ç‚¹**ï¼š
- æ¯” LayerNorm æ›´é«˜æ•ˆï¼ˆå»æ‰å‡å€¼ä¸­å¿ƒåŒ–ï¼‰
- åªæœ‰ weight å‚æ•°ï¼Œæ²¡æœ‰ bias
- é»˜è®¤ eps = 1e-6

---

### ä¸‰ã€Transformer ç»„ä»¶å®Œå–„ âœ…

#### 1. MultiHeadAttention å¢å¼º

**æ–°å¢åŠŸèƒ½**ï¼š
- âœ… æ”¯æŒ `attnMask`ï¼ˆæ³¨æ„åŠ›æ©ç ï¼Œå¦‚å› æœæ©ç ï¼‰
- âœ… æ”¯æŒ `keyPaddingMask`ï¼ˆé”®å¡«å……æ©ç ï¼‰
- âœ… æ”¯æŒä¸åŒé•¿åº¦çš„ query/key/value åºåˆ—

**æ–°å¢é™æ€æ–¹æ³•**ï¼š
```java
// ç”Ÿæˆå› æœæ©ç 
Variable causalMask = MultiHeadAttention.generateCausalMask(seqLen);

// ç”Ÿæˆå¯å¹¿æ’­çš„å› æœæ©ç 
Variable causalMaskBatched = MultiHeadAttention.generateCausalMaskBatched(seqLen);

// ç”Ÿæˆå¡«å……æ©ç 
Variable paddingMask = MultiHeadAttention.generatePaddingMask(batchSize, maxLen, actualLengths);

// ç»„åˆå› æœæ©ç å’Œå¡«å……æ©ç 
Variable combinedMask = MultiHeadAttention.combineCausalAndPaddingMask(seqLen, paddingMask);
```

#### 2. TransformerEncoder å®¹å™¨

**æ–‡ä»¶**: `layer/transformer/TransformerEncoder.java`

**åŠŸèƒ½**ï¼š
- å †å å¤šä¸ª `TransformerEncoderLayer`
- æ”¯æŒå¯é€‰çš„æœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼ˆPre-LNæ¶æ„ï¼‰
- æ”¯æŒæºåºåˆ—æ©ç 

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```java
TransformerEncoder encoder = new TransformerEncoder(
    "encoder",
    numLayers,    // å±‚æ•°
    dModel,       // æ¨¡å‹ç»´åº¦
    numHeads,     // æ³¨æ„åŠ›å¤´æ•°
    dFF,          // FFNéšè—å±‚ç»´åº¦
    dropout,      // dropoutæ¯”ç‡
    preLayerNorm  // æ˜¯å¦Pre-LN
);

Variable output = encoder.forward(src, srcMask);
```

#### 3. TransformerDecoder å®¹å™¨

**æ–‡ä»¶**: `layer/transformer/TransformerDecoder.java`

**åŠŸèƒ½**ï¼š
- å †å å¤šä¸ª `TransformerDecoderLayer`
- æ”¯æŒå¯é€‰çš„æœ€ç»ˆå±‚å½’ä¸€åŒ–
- æ”¯æŒç›®æ ‡åºåˆ—æ©ç ï¼ˆå› æœæ©ç ï¼‰
- æ”¯æŒç¼–ç å™¨è¾“å‡ºæ©ç 

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```java
TransformerDecoder decoder = new TransformerDecoder(
    "decoder",
    numLayers,
    dModel,
    numHeads
);

Variable output = decoder.forward(tgt, memory, tgtMask);
```

#### 4. å®Œæ•´ Transformer æ¨¡å‹

**æ–‡ä»¶**: `layer/transformer/Transformer.java`

**åŠŸèƒ½**ï¼š
- ç»„åˆ Encoder + Decoder
- æ”¯æŒåˆ†ç¦»çš„ encode/decode æ–¹æ³•ï¼ˆç”¨äºæ¨ç†ï¼‰
- æä¾›ç”Ÿæˆå› æœæ©ç çš„ä¾¿æ·æ–¹æ³•

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```java
// åˆ›å»ºå®Œæ•´Transformer
Transformer transformer = new Transformer(
    "transformer",
    dModel,           // 512
    numHeads,         // 8
    numEncoderLayers, // 6
    numDecoderLayers  // 6
);

// è®­ç»ƒæ—¶ï¼šè”åˆç¼–è§£ç 
Variable output = transformer.forward(src, tgt, tgtMask);

// æ¨ç†æ—¶ï¼šåˆ†ç¦»å¼
Variable memory = transformer.encode(src);
Variable output = transformer.decode(tgt, memory, causalMask);
```

---

### å››ã€Module ç±»å¢å¼º âœ…

**æ–°å¢æ–¹æ³•**ï¼š

| æ–¹æ³• | åŠŸèƒ½ | è¿”å›ç±»å‹ |
|------|------|---------|
| `freeze()` | å†»ç»“æ‰€æœ‰å‚æ•° | Module |
| `unfreeze()` | è§£å†»æ‰€æœ‰å‚æ•° | Module |
| `requiresGrad(boolean)` | è®¾ç½®æ˜¯å¦éœ€è¦æ¢¯åº¦ | Module |
| `numParameters(boolean)` | ç»Ÿè®¡å‚æ•°æ•°é‡ | long |
| `numParameters()` | ç»Ÿè®¡æ‰€æœ‰å‚æ•°æ•°é‡ | long |
| `parameterSummary()` | è·å–å‚æ•°æ‘˜è¦ | String |
| `copyStateDict()` | æ·±æ‹·è´çŠ¶æ€å­—å…¸ | Map |
| `extraRepr()` | é¢å¤–ä¿¡æ¯è¡¨ç¤º | String |
| `numChildren()` | å­æ¨¡å—æ•°é‡ | int |
| `children()` | æ‰€æœ‰ç›´æ¥å­æ¨¡å— | Collection |
| `modules()` | æ‰€æœ‰æ¨¡å—ï¼ˆå«è‡ªèº«ï¼‰ | Iterable |
| `evalAndFreeze()` | è¯„ä¼°æ¨¡å¼+å†»ç»“ | Module |
| `trainAndUnfreeze()` | è®­ç»ƒæ¨¡å¼+è§£å†» | Module |

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```java
// ç»Ÿè®¡å‚æ•°
System.out.println("Total params: " + model.numParameters());
System.out.println("Trainable params: " + model.numParameters(true));

// å‚æ•°æ‘˜è¦
System.out.println(model.parameterSummary());

// å†»ç»“/è§£å†»
model.freeze();  // å†»ç»“æ‰€æœ‰å‚æ•°
model.getModule("encoder").freeze();  // åªå†»ç»“ç¼–ç å™¨
model.unfreeze();  // è§£å†»æ‰€æœ‰å‚æ•°

// æ¨ç†æ¨¡å¼
model.evalAndFreeze();
```

---

### äº”ã€Functional API æ‰©å±• âœ…

**æ–°å¢æ¿€æ´»å‡½æ•°**ï¼š
```java
Functional.gelu(input)
Functional.silu(input)
Functional.leakyRelu(input, negativeSlope)
Functional.elu(input, alpha)
Functional.logSoftmax(input, axis)
```

**æ–°å¢å½’ä¸€åŒ–**ï¼š
```java
Functional.rmsNorm(input, weight, eps)
```

**æ–°å¢æ³¨æ„åŠ›**ï¼š
```java
Functional.scaledDotProductAttention(query, key, value, attnMask, dropout, training)
```

**æ–°å¢æŸå¤±å‡½æ•°**ï¼š
```java
Functional.crossEntropyLoss(input, target)
Functional.nllLoss(input, target)
Functional.mseLoss(input, target)
Functional.binaryCrossEntropyLoss(input, target)
Functional.binaryCrossEntropyWithLogitsLoss(input, target)
```

---

## æ–°å¢æ–‡ä»¶åˆ—è¡¨

### åº•å±‚ func æ¨¡å—
```
tinyai-deeplearning-func/src/main/java/io/leavesfly/tinyai/func/math/
â”œâ”€â”€ SiLU.java          ğŸ†•
â”œâ”€â”€ LeakyReLU.java     ğŸ†•
â”œâ”€â”€ ELU.java           ğŸ†•
â””â”€â”€ LogSoftmax.java    ğŸ†•
```

### V2 å±‚å®ç°
```
tinyai-deeplearning-nnet/src/main/java/io/leavesfly/tinyai/nnet/v2/layer/
â”œâ”€â”€ activation/
â”‚   â”œâ”€â”€ GELU.java           ğŸ†•
â”‚   â”œâ”€â”€ SiLU.java           ğŸ†•
â”‚   â”œâ”€â”€ LeakyReLU.java      ğŸ†•
â”‚   â”œâ”€â”€ ELU.java            ğŸ†•
â”‚   â””â”€â”€ LogSoftmax.java     ğŸ†•
â”œâ”€â”€ norm/
â”‚   â””â”€â”€ RMSNorm.java        ğŸ†•
â””â”€â”€ transformer/
    â”œâ”€â”€ MultiHeadAttention.java   âœï¸ å¢å¼º
    â”œâ”€â”€ TransformerEncoder.java   ğŸ†•
    â”œâ”€â”€ TransformerDecoder.java   ğŸ†•
    â””â”€â”€ Transformer.java          ğŸ†•
```

### ä¿®æ”¹çš„ç°æœ‰æ–‡ä»¶
```
tinyai-deeplearning-func/src/main/java/io/leavesfly/tinyai/func/Variable.java  âœï¸
tinyai-deeplearning-nnet/src/main/java/io/leavesfly/tinyai/nnet/v2/core/Module.java  âœï¸
tinyai-deeplearning-nnet/src/main/java/io/leavesfly/tinyai/nnet/v2/functional/Functional.java  âœï¸
```

---

## ä¸ PyTorch å¯¹æ ‡

| PyTorch | TinyAI V2 | çŠ¶æ€ |
|---------|-----------|------|
| `nn.GELU` | `layer.activation.GELU` | âœ… |
| `nn.SiLU` | `layer.activation.SiLU` | âœ… |
| `nn.LeakyReLU` | `layer.activation.LeakyReLU` | âœ… |
| `nn.ELU` | `layer.activation.ELU` | âœ… |
| `nn.LogSoftmax` | `layer.activation.LogSoftmax` | âœ… |
| `nn.RMSNorm` | `layer.norm.RMSNorm` | âœ… |
| `nn.MultiheadAttention` | `layer.transformer.MultiHeadAttention` | âœ… å¢å¼º |
| `nn.TransformerEncoder` | `layer.transformer.TransformerEncoder` | âœ… |
| `nn.TransformerDecoder` | `layer.transformer.TransformerDecoder` | âœ… |
| `nn.Transformer` | `layer.transformer.Transformer` | âœ… |
| `Module.freeze()` | `Module.freeze()` | âœ… |
| `Module.parameters()` | `Module.namedParameters()` | âœ… |
| `F.gelu` | `Functional.gelu` | âœ… |
| `F.scaled_dot_product_attention` | `Functional.scaledDotProductAttention` | âœ… |
| `F.cross_entropy` | `Functional.crossEntropyLoss` | âœ… |

---

## åç»­è®¡åˆ’

### çŸ­æœŸ
- [ ] å®Œå–„å•å…ƒæµ‹è¯•
- [ ] æ·»åŠ æ›´å¤šæ¿€æ´»å‡½æ•° (Mish, Hardswish, PReLU)
- [ ] æ·»åŠ  GroupNorm, BatchNorm2d

### ä¸­æœŸ
- [ ] RNN å±‚å¢å¼ºï¼ˆå¤šå±‚ã€åŒå‘ï¼‰
- [ ] æ·»åŠ  LSTMCell, GRUCell
- [ ] Conv1d, ConvTranspose2d

### é•¿æœŸ
- [ ] ä½ç½®ç¼–ç æ‰©å±• (RoPE, ALiBi)
- [ ] Flash Attention ä¼˜åŒ–
- [ ] æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ

---

## ä½¿ç”¨å»ºè®®

1. **ç°ä»£ LLM å¼€å‘**ï¼šä½¿ç”¨ `GELU` + `RMSNorm` + `Transformer`
2. **å›¾åƒåˆ†ç±»**ï¼šä½¿ç”¨ `SiLU` + ç°æœ‰å·ç§¯å±‚
3. **è¿ç§»å­¦ä¹ **ï¼šä½¿ç”¨ `freeze()` / `unfreeze()` ç®¡ç†å‚æ•°
4. **æ¨¡å‹åˆ†æ**ï¼šä½¿ç”¨ `parameterSummary()` æŸ¥çœ‹æ¨¡å‹ç»“æ„

---

**æ›´æ–°æ—¶é—´**: 2024å¹´
**ç‰ˆæœ¬**: V2.1

