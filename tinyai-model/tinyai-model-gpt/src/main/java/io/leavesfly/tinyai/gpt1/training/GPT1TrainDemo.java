package io.leavesfly.tinyai.gpt1.training;


import io.leavesfly.tinyai.gpt1.GPT1Config;
import io.leavesfly.tinyai.gpt1.GPT1Model;

import java.util.Arrays;
import java.util.List;

/**
 * GPT-1è®­ç»ƒå’Œæ¨ç†å®Œæ•´æ¼”ç¤º
 * <p>
 * å±•ç¤ºå®Œæ•´çš„è®­ç»ƒæµç¨‹:
 * 1. é¢„è®­ç»ƒ(Pretrain)
 * 2. å¾®è°ƒ(Finetune/Posttrain)
 * 3. æ¨ç†(Inference)
 *
 * @author TinyAI
 * @since 2024
 */
public class GPT1TrainDemo {

    private static List<String> preTrainTexts = Arrays.asList(
            // æ·±åº¦å­¦ä¹ åŸºç¡€
            "Deep learning is a subset of machine learning",
            "Deep learning uses neural networks with multiple layers",
            "Deep learning can learn complex patterns from data",
            "Deep learning is transforming artificial intelligence",
            "Deep learning models require large amounts of data",
            "Deep learning has achieved remarkable success in many fields",
            "Deep learning algorithms can automatically extract features",
            "Deep learning is the foundation of modern AI systems",
            "Deep learning enables end to end learning",
            "Deep learning models are trained on GPUs",
            // æœºå™¨å­¦ä¹ 
            "Machine learning algorithms improve with experience",
            "Machine learning is a branch of artificial intelligence",
            "Machine learning enables computers to learn from data",
            "Machine learning models can make predictions",
            "Machine learning requires feature engineering and data preprocessing",
            "Machine learning is used in recommendation systems",
            "Machine learning powers search engines and spam filters",
            "Supervised learning uses labeled training data",
            "Unsupervised learning finds patterns without labels",
            "Reinforcement learning learns through trial and error",
            // ç¥ç»ç½‘ç»œ
            "Neural networks learn patterns from data",
            "Neural networks have multiple layers of neurons",
            "Neural networks can approximate any function",
            "Neural networks are inspired by the human brain",
            "Neural networks consist of input output and hidden layers",
            "Neural networks use activation functions for nonlinearity",
            "Neural networks are trained using backpropagation",
            "Neural networks can process images text and speech",
            "Convolutional neural networks excel at image processing",
            "Recurrent neural networks handle sequential data",
            // è‡ªç„¶è¯­è¨€å¤„ç†
            "Natural language processing enables computers to understand text",
            "Natural language processing is used in chatbots",
            "Natural language processing powers machine translation",
            "Natural language processing includes sentiment analysis",
            "Natural language processing helps computers read and write",
            "Language models can generate coherent text",
            "Language models learn from large text corpora",
            "Language models predict the next word in a sequence",
            "Language models are the core of modern NLP systems",
            "Word embeddings represent words as dense vectors",
            "Tokenization splits text into smaller units",
            "Text generation creates new content from prompts",
            // Transformerå’ŒGPT
            "Transformer architecture revolutionized NLP",
            "Transformer models use attention mechanisms",
            "Attention is all you need for sequence modeling",
            "GPT uses transformer decoder architecture",
            "GPT generates text in an autoregressive manner",
            "GPT learns from massive amounts of text data",
            "GPT can perform many NLP tasks without fine tuning",
            "The attention mechanism computes weighted relationships",
            "Self attention allows the model to focus on relevant parts",
            "Multi head attention captures different aspects",
            "Position embeddings encode sequence order",
            "Layer normalization stabilizes training",
            // äººå·¥æ™ºèƒ½
            "Artificial intelligence is transforming the world",
            "Artificial intelligence can solve complex problems",
            "AI systems learn from experience and data",
            "AI is used in many applications today",
            "AI enables automation and intelligent decision making",
            "AI is reshaping industries and creating new opportunities",
            "AI research focuses on creating intelligent machines",
            "AI applications include robotics and autonomous vehicles",
            "General AI aims to match human intelligence",
            "Narrow AI excels at specific tasks",
            // è®­ç»ƒä¸ä¼˜åŒ–
            "Training neural networks requires gradient descent",
            "Optimization algorithms minimize the loss function",
            "Batch size affects training speed and convergence",
            "Learning rate is a critical hyperparameter",
            "Regularization prevents overfitting",
            "Dropout is a common regularization technique",
            "Pretrained models can be fine tuned for specific tasks",
            "Transfer learning enables knowledge reuse",
            "Early stopping prevents overfitting during training",
            "Data augmentation increases training data variety",
            "Cross validation helps evaluate model performance",
            "Hyperparameter tuning optimizes model settings",
            // åº”ç”¨åœºæ™¯
            "Image recognition uses convolutional neural networks",
            "Speech recognition converts audio to text",
            "Computer vision enables machines to see",
            "Text classification assigns labels to documents",
            "Named entity recognition extracts information from text",
            "Question answering systems provide accurate responses",
            "Sentiment analysis determines emotional tone",
            "Machine translation converts text between languages",
            "Object detection locates items in images",
            "Face recognition identifies people in photos",
            "Voice assistants use speech recognition and NLP",
            "Recommendation engines suggest relevant content",
            // æ•°æ®ä¸ç‰¹å¾
            "Data is the fuel for machine learning",
            "Feature extraction identifies important patterns",
            "Data cleaning removes noise and errors",
            "Feature scaling normalizes input values",
            "Dimensionality reduction simplifies complex data",
            "Data visualization helps understand patterns",
            "Labeled data is essential for supervised learning",
            "Big data enables training of large models",
            // æ¨¡å‹è¯„ä¼°
            "Accuracy measures correct predictions",
            "Precision and recall evaluate classification",
            "Loss function quantifies prediction errors",
            "Validation data helps tune hyperparameters",
            "Test data evaluates final model performance",
            "Confusion matrix shows classification results",
            "ROC curve plots true and false positive rates",
            "F1 score balances precision and recall"
    );


    private static
    List<String> finetuneTexts = Arrays.asList(
            // åŸºç¡€æ¦‚å¿µQA
            "Question: What is deep learning? Answer: Deep learning is a type of machine learning using neural networks",
            "Question: What is NLP? Answer: NLP stands for natural language processing",
            "Question: What is AI? Answer: AI is artificial intelligence",
            "Question: What are neural networks? Answer: Neural networks are computing systems inspired by the brain",
            "Question: What is machine learning? Answer: Machine learning enables computers to learn from data",
            "Question: What is a transformer? Answer: A transformer is a neural network architecture using attention",
            "Question: What is GPT? Answer: GPT is a generative pretrained transformer for text generation",
            "Question: What is attention? Answer: Attention is a mechanism to focus on relevant parts of input",
            // æŠ€æœ¯QA
            "Question: What is backpropagation? Answer: Backpropagation is an algorithm for training neural networks",
            "Question: What is gradient descent? Answer: Gradient descent is an optimization algorithm",
            "Question: What is overfitting? Answer: Overfitting is when a model memorizes training data",
            "Question: What is regularization? Answer: Regularization prevents overfitting in models",
            "Question: What is dropout? Answer: Dropout randomly disables neurons during training",
            "Question: What is transfer learning? Answer: Transfer learning reuses knowledge from pretrained models",
            "Question: What is fine tuning? Answer: Fine tuning adapts pretrained models to new tasks",
            "Question: What is tokenization? Answer: Tokenization splits text into smaller units",
            // åº”ç”¨QA
            "Question: What is image recognition? Answer: Image recognition identifies objects in images",
            "Question: What is speech recognition? Answer: Speech recognition converts audio to text",
            "Question: What is sentiment analysis? Answer: Sentiment analysis detects emotional tone in text",
            "Question: What is machine translation? Answer: Machine translation converts text between languages",
            "Question: What is text classification? Answer: Text classification assigns labels to documents",
            "Question: What is named entity recognition? Answer: NER extracts entities from text",
            // æ¨¡å‹QA
            "Question: What is CNN? Answer: CNN is convolutional neural network for image processing",
            "Question: What is RNN? Answer: RNN is recurrent neural network for sequential data",
            "Question: What is LSTM? Answer: LSTM is long short term memory for learning sequences",
            "Question: What is embedding? Answer: Embedding represents words as dense vectors",
            "Question: What is softmax? Answer: Softmax converts logits to probability distribution",
            "Question: What is loss function? Answer: Loss function measures prediction errors"
    );

    private static
    List<String> finetuneValTexts = Arrays.asList(
            "Question: What is machine learning? Answer: Machine learning enables computers to learn",
            "Question: What is fine tuning? Answer: Fine tuning adapts pretrained models to new tasks",
            "Question: What is tokenization? Answer: Tokenization splits text into smaller units",
            "Question: What is machine learning? Answer: Machine learning enables computers to learn from data",
            "Question: What is embedding? Answer: Embedding represents words as dense vectors"
    );

    // å…±äº«çš„tokenizerï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨åŒä¸€ä¸ª
    private static GPT1Dataset.SimpleTokenizer sharedTokenizer = new GPT1Dataset.SimpleTokenizer();

    public static void main(String[] args) {
        System.out.println("=".repeat(70));
        System.out.println("GPT-1 å®Œæ•´è®­ç»ƒä¸æ¨ç†æ¼”ç¤º");
        System.out.println("=".repeat(70));

        try {
            // æ¼”ç¤º1: é¢„è®­ç»ƒ
            GPT1Model pretrainedModel = demoPretraining();

            // æ¼”ç¤º2: å¾®è°ƒ(ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹)
            GPT1Model finetunedModel = demoFinetuning(pretrainedModel);

            // æ¼”ç¤º3: æ¨ç†
            demoInference(finetunedModel);

            System.out.println("\n" + "=".repeat(70));
            System.out.println("âœ… æ¼”ç¤ºå®Œæˆ!");
            System.out.println("=".repeat(70));
        } catch (Exception e) {
            System.err.println("æ¼”ç¤ºè¿‡ç¨‹å‡ºé”™: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * æ¼”ç¤º1: é¢„è®­ç»ƒæµç¨‹
     *
     * @return é¢„è®­ç»ƒåçš„æ¨¡å‹
     */
    private static GPT1Model demoPretraining() {
        System.out.println("\n" + "=".repeat(70));
        System.out.println("ğŸ“š æ¼”ç¤º1: GPT-1é¢„è®­ç»ƒ (Pretrain)");
        System.out.println("=".repeat(70));

        // 1. å‡†å¤‡æ•°æ®é›†(å…ˆæ„å»ºè¯æ±‡è¡¨)
        System.out.println("\nğŸ“ æ­¥éª¤1: å‡†å¤‡é¢„è®­ç»ƒæ•°æ®");


        // å…ˆç”¨æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨(åŒ…å«é¢„è®­ç»ƒ+å¾®è°ƒæ‰€æœ‰æ•°æ®)
        for (String text : preTrainTexts) {
            sharedTokenizer.encode(text);
        }

        for (String text : finetuneTexts) {
            sharedTokenizer.encode(text);
        }
        int actualVocabSize = sharedTokenizer.getVocabSize();
        System.out.println("âœ“ è¯æ±‡è¡¨æ„å»ºå®Œæˆ");
        System.out.println("  - è¯æ±‡è¡¨å¤§å°: " + actualVocabSize);

        // 2. åˆ›å»ºæ¨¡å‹(ä½¿ç”¨å®é™…è¯æ±‡è¡¨å¤§å°)
        System.out.println("\nğŸ“ æ­¥éª¤2: åˆ›å»ºæ¨¡å‹");
        GPT1Config config = GPT1Config.createTinyConfig();
        config.setVocabSize(actualVocabSize);  // è®¾ç½®å®é™…è¯æ±‡è¡¨å¤§å°
        GPT1Model model = new GPT1Model("gpt1-pretrain-demo", config);
        System.out.println("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ");
        System.out.println("  - é…ç½®: Tiny");
        System.out.println("  - è¯æ±‡è¡¨å¤§å°: " + config.getVocabSize());
        System.out.println("  - éšè—ç»´åº¦: " + config.getNEmbd());
        System.out.println("  - å±‚æ•°: " + config.getNLayer());
        System.out.println("  - æ³¨æ„åŠ›å¤´: " + config.getNHead());

        // 3. åŠ è½½æ•°æ®é›†
        System.out.println("\nğŸ“ æ­¥éª¤3: åŠ è½½æ•°æ®é›†");
        GPT1Dataset dataset = new GPT1Dataset(
                config.getNPositions(),  // maxSeqLen
                2,                       // batchSize(å‡å°ä»¥èŠ‚çœå†…å­˜)
                actualVocabSize          // vocabSize
        );
        dataset.loadFromTexts(preTrainTexts, sharedTokenizer);
        System.out.println("âœ“ æ•°æ®åŠ è½½å®Œæˆ");
        System.out.println("  - æ ·æœ¬æ•°: " + dataset.getSampleCount());

        // 4. é…ç½®å¹¶å¼€å§‹é¢„è®­ç»ƒ
        System.out.println("\nğŸ“ æ­¥éª¤4: å¼€å§‹é¢„è®­ç»ƒ");
        GPT1Pretrain trainer = new GPT1Pretrain(model, dataset);
        trainer.configure(
                5,        // maxEpochs
                5e-3f,    // learningRate
                20,       // warmupSteps
                1.0f      // maxGradNorm
        ).setCheckpoint("./checkpoints/pretrain_demo", 500);

        System.out.println("å¼€å§‹è®­ç»ƒ...");
        trainer.train();

        System.out.println("\nâœ… é¢„è®­ç»ƒå®Œæˆ!");

        return model;
    }

    /**
     * æ¼”ç¤º2: å¾®è°ƒæµç¨‹
     *
     * @param pretrainedModel é¢„è®­ç»ƒçš„æ¨¡å‹
     * @return å¾®è°ƒåçš„æ¨¡å‹
     */
    private static GPT1Model demoFinetuning(GPT1Model pretrainedModel) {
        System.out.println("\n" + "=".repeat(70));
        System.out.println("ğŸ¯ æ¼”ç¤º2: GPT-1å¾®è°ƒ (Finetune/Posttrain)");
        System.out.println("=".repeat(70));

        // 1. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        System.out.println("\nğŸ“ æ­¥éª¤1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹");
        GPT1Model model = pretrainedModel;
        GPT1Config config = model.getConfig();
        System.out.println("âœ“ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ");
        System.out.println("  - æ¨¡å‹åç§°: " + model.getName());
        System.out.println("  - å‚æ•°é‡: " + model.getAllParams().size());

        // 2. å‡†å¤‡å¾®è°ƒæ•°æ®
        System.out.println("\nğŸ“ æ­¥éª¤2: å‡†å¤‡å¾®è°ƒæ•°æ®");


        GPT1Dataset trainDataset = new GPT1Dataset(
                config.getNPositions(), 2, sharedTokenizer.getVocabSize() + 10
        );
        trainDataset.loadFromTexts(finetuneTexts, sharedTokenizer);

        GPT1Dataset valDataset = new GPT1Dataset(
                config.getNPositions(), 1, sharedTokenizer.getVocabSize() + 10
        );
        valDataset.loadFromTexts(finetuneValTexts, sharedTokenizer);

        System.out.println("âœ“ å¾®è°ƒæ•°æ®å‡†å¤‡å®Œæˆ");
        System.out.println("  - è®­ç»ƒæ ·æœ¬: " + trainDataset.getSampleCount());
        System.out.println("  - éªŒè¯æ ·æœ¬: " + valDataset.getSampleCount());

        // 3. é…ç½®å¹¶å¼€å§‹å¾®è°ƒ
        System.out.println("\nğŸ“ æ­¥éª¤3: å¼€å§‹å¾®è°ƒ");
        GPT1Finetune finetuner = new GPT1Finetune(model, trainDataset, valDataset);
        finetuner.configure(
                2,        // maxEpochs
                5e-4f,    // learningRate(æ¯”é¢„è®­ç»ƒå°)
                2         // patience
        ).setCheckpoint("./checkpoints/finetune_demo", 50);

        // å®é™…æ‰§è¡Œå¾®è°ƒ
        System.out.println("å¼€å§‹å¾®è°ƒ...");
        finetuner.train();

        System.out.println("\nâœ… å¾®è°ƒå®Œæˆ!");
        System.out.println("\nğŸ“Š å¾®è°ƒé˜¶æ®µè¯´æ˜:");
        System.out.println("  - ç›®æ ‡: é€‚åº”ç‰¹å®šä»»åŠ¡");
        System.out.println("  - æ•°æ®: ä»»åŠ¡ç›¸å…³çš„æ ‡æ³¨æ•°æ®");
        System.out.println("  - æŸå¤±: ä»»åŠ¡ç‰¹å®šæŸå¤±");
        System.out.println("  - å­¦ä¹ ç‡: æ¯”é¢„è®­ç»ƒå°");
        System.out.println("  - æŠ€å·§: æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ");

        return model;
    }

    /**
     * æ¼”ç¤º3: æ¨ç†æµç¨‹
     *
     * @param model è®­ç»ƒå¥½çš„æ¨¡å‹
     */
    private static void demoInference(GPT1Model model) {
        System.out.println("\n" + "=".repeat(70));
        System.out.println("ğŸš€ æ¼”ç¤º3: GPT-1æ¨ç†ä¸æ–‡æœ¬ç”Ÿæˆ");
        System.out.println("=".repeat(70));

        // 1. å‡†å¤‡æ¨ç†å™¨
        System.out.println("\nğŸ“ æ­¥éª¤1: å‡†å¤‡æ¨ç†å™¨");
        GPT1Inference inference = new GPT1Inference(model);
        System.out.println("âœ“ æ¨ç†å™¨å‡†å¤‡å®Œæˆ");

        // 2. å‡†å¤‡æç¤ºè¯(ä½¿ç”¨å…±äº«çš„tokenizer)
        System.out.println("\nğŸ“ æ­¥éª¤2: å‡†å¤‡æç¤ºè¯");
        String promptText = "Deep learning is";
        List<Integer> promptTokens = sharedTokenizer.encode(promptText);
        int[] promptIds = promptTokens.stream().mapToInt(i -> i).toArray();

        System.out.println("âœ“ æç¤ºæ–‡æœ¬: \"" + promptText + "\"");
        System.out.println("  - Tokenåºåˆ—: " + Arrays.toString(promptIds));
        System.out.println("  - Tokenæ•°é‡: " + promptIds.length);
        System.out.println("  - è¯æ±‡è¡¨å¤§å°: " + sharedTokenizer.getVocabSize());

        // 3. æ‰§è¡Œå®é™…æ–‡æœ¬ç”Ÿæˆ
        System.out.println("\nğŸ“ æ­¥éª¤3: æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º\n");

        // ç­–ç•¥1: è´ªå©ªè§£ç 
        System.out.println("ç­–ç•¥1: è´ªå©ªè§£ç  (Greedy Decoding)");
        System.out.println("  - ç‰¹ç‚¹: å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token");
        System.out.println("  - ä¼˜ç‚¹: ç¡®å®šæ€§è¾“å‡º,é€‚åˆéœ€è¦ä¸€è‡´æ€§çš„ä»»åŠ¡");
        System.out.println("  - ç¼ºç‚¹: å¯èƒ½é™·å…¥é‡å¤æ¨¡å¼");
        try {
            int[] greedyResult = inference.generateGreedy(promptIds, 10);
            String greedyText = sharedTokenizer.decode(greedyResult);
            System.out.println("  âœ“ ç”Ÿæˆç»“æœ: \"" + greedyText + "\"");
        } catch (Exception e) {
            System.out.println("  âš  ç”Ÿæˆè·³è¿‡: " + e.getMessage());
        }

        // ç­–ç•¥2: Temperatureé‡‡æ ·
        System.out.println("\nç­–ç•¥2: Temperatureé‡‡æ ·");
        System.out.println("  - å‚æ•°: temperature=0.8");
        System.out.println("  - ç‰¹ç‚¹: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§");
        System.out.println("  - temperature<1: æ›´ç¡®å®šæ€§");
        System.out.println("  - temperature>1: æ›´éšæœºæ€§");
        try {
            int[] tempResult = inference.generateWithTemperature(promptIds, 10, 0.8f);
            String tempText = sharedTokenizer.decode(tempResult);
            System.out.println("  âœ“ ç”Ÿæˆç»“æœ: \"" + tempText + "\"");
        } catch (Exception e) {
            System.out.println("  âš  ç”Ÿæˆè·³è¿‡: " + e.getMessage());
        }

        // ç­–ç•¥3: Beam Search
        System.out.println("\nç­–ç•¥3: Beam Search");
        System.out.println("  - å‚æ•°: beamSize=3");
        System.out.println("  - ç‰¹ç‚¹: ç»´æŠ¤å¤šä¸ªå€™é€‰åºåˆ—,é€‰æ‹©å…¨å±€æœ€ä¼˜");
        System.out.println("  - ä¼˜ç‚¹: ç”Ÿæˆè´¨é‡é«˜");
        System.out.println("  - ç¼ºç‚¹: è®¡ç®—å¼€é”€å¤§");
        try {
            int[] beamResult = inference.generateBeamSearch(promptIds, 10, 3);
            String beamText = sharedTokenizer.decode(beamResult);
            System.out.println("  âœ“ ç”Ÿæˆç»“æœ: \"" + beamText + "\"");
        } catch (Exception e) {
            System.out.println("  âš  ç”Ÿæˆè·³è¿‡: " + e.getMessage());
        }

        System.out.println("\nğŸ’¡ æ¨ç†é˜¶æ®µè¯´æ˜:");
        System.out.println("  - è¾“å…¥: æç¤ºè¯tokenåºåˆ—");
        System.out.println("  - è¾“å‡º: ç”Ÿæˆçš„tokenåºåˆ—");
        System.out.println("  - ç­–ç•¥é€‰æ‹©:");
        System.out.println("    * éœ€è¦ç¡®å®šæ€§: è´ªå©ªè§£ç ");
        System.out.println("    * å¹³è¡¡è´¨é‡ä¸å¤šæ ·æ€§: Temperatureé‡‡æ ·");
        System.out.println("    * æœ€é«˜è´¨é‡: Beam Search");
        System.out.println("    * åˆ›é€ æ€§ä»»åŠ¡: é«˜temperatureçš„é‡‡æ ·");
    }

    /**
     * å®Œæ•´æµç¨‹æ¼”ç¤º
     */
    public static void runCompleteWorkflow() {
        System.out.println("=".repeat(70));
        System.out.println("GPT-1 å®Œæ•´è®­ç»ƒæµç¨‹");
        System.out.println("=".repeat(70));

        // é˜¶æ®µ1: é¢„è®­ç»ƒ
        System.out.println("\né˜¶æ®µ1: é¢„è®­ç»ƒ (Pretrain)");
        System.out.println("  ç›®æ ‡: å­¦ä¹ è¯­è¨€çš„é€šç”¨è¡¨ç¤º");
        System.out.println("  æ•°æ®: BooksCorpus (7000æœ¬ä¹¦ç±)");
        System.out.println("  ä»»åŠ¡: å› æœè¯­è¨€å»ºæ¨¡ (é¢„æµ‹ä¸‹ä¸€ä¸ªè¯)");
        System.out.println("  è€—æ—¶: çº¦30å¤© (8ä¸ªGPU)");

        // é˜¶æ®µ2: å¾®è°ƒ
        System.out.println("\né˜¶æ®µ2: å¾®è°ƒ (Finetune/Posttrain)");
        System.out.println("  ç›®æ ‡: é€‚åº”ä¸‹æ¸¸ä»»åŠ¡");
        System.out.println("  æ•°æ®: ä»»åŠ¡ç‰¹å®šæ•°æ®é›†");
        System.out.println("  ä»»åŠ¡: æ–‡æœ¬åˆ†ç±»/é—®ç­”/æ–‡æœ¬è•´å«ç­‰");
        System.out.println("  è€—æ—¶: çº¦3ä¸ªepoch");

        // é˜¶æ®µ3: æ¨ç†
        System.out.println("\né˜¶æ®µ3: æ¨ç† (Inference)");
        System.out.println("  è¾“å…¥: æç¤ºè¯");
        System.out.println("  å¤„ç†: è‡ªå›å½’ç”Ÿæˆ");
        System.out.println("  è¾“å‡º: ç”Ÿæˆæ–‡æœ¬");
        System.out.println("  é€Ÿåº¦: æ¯«ç§’çº§ (CPUæ¨ç†)");

        System.out.println("\nè®­ç»ƒæç¤º:");
        System.out.println("  1. é¢„è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æº");
        System.out.println("  2. å¾®è°ƒå¯ä»¥åœ¨å•å¡ä¸Šå®Œæˆ");
        System.out.println("  3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å¯ä»¥æ¨¡æ‹Ÿæ›´å¤§çš„batch");
        System.out.println("  4. å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹é˜²æ­¢è®­ç»ƒä¸­æ–­");
        System.out.println("  5. ç›‘æ§éªŒè¯é›†æŸå¤±é˜²æ­¢è¿‡æ‹Ÿåˆ");
    }
}
