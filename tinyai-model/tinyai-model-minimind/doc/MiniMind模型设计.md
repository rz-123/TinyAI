# MiniMind æ¨¡å‹è®¾è®¡æ–‡æ¡£

## ğŸ“ è®¾è®¡æ¦‚è§ˆ

### è®¾è®¡ç†å¿µ

MiniMind æ˜¯ä¸€ä¸ª**æè‡´è½»é‡åŒ–**çš„è¯­è¨€æ¨¡å‹,ä»… **26M å‚æ•°**,ä½†å…·å¤‡å®Œæ•´çš„ Transformer æ¶æ„å’Œç°ä»£ LLM çš„æ ¸å¿ƒèƒ½åŠ›ã€‚è®¾è®¡ç†å¿µ:

1. **æ•™è‚²å‹å¥½**: æ¸…æ™°çš„æ¶æ„,æ˜“äºç†è§£å’Œå­¦ä¹ 
2. **èµ„æºé«˜æ•ˆ**: ä½å‚æ•°é‡,æ™®é€šç¡¬ä»¶å¯è®­ç»ƒå’Œéƒ¨ç½²
3. **åŠŸèƒ½å®Œæ•´**: æ”¯æŒé¢„è®­ç»ƒã€SFTã€LoRAã€DPO ç­‰å…¨æµç¨‹
4. **å¯æ‰©å±•æ€§**: æ”¯æŒ MoE æ¶æ„å’Œå¤šæ¨¡æ€æ‰©å±•

### æ¶æ„ç‰¹ç‚¹

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **Decoder-Only** | GPTé£æ ¼çš„å•å‘è§£ç å™¨æ¶æ„ |
| **Multi-Head Attention** | 16ä¸ªæ³¨æ„åŠ›å¤´,å¤´ç»´åº¦32 |
| **RoPEä½ç½®ç¼–ç ** | æ—‹è½¬ä½ç½®åµŒå…¥,æ”¯æŒé•¿åº¦å¤–æ¨ |
| **Pre-LayerNorm** | å±‚å‰å½’ä¸€åŒ–,è®­ç»ƒæ›´ç¨³å®š |
| **SiLUæ¿€æ´»** | Swishæ¿€æ´»å‡½æ•°,æ€§èƒ½ä¼˜äºReLU |
| **å› æœæ©ç ** | è‡ªå›å½’ç”Ÿæˆ,ç¦æ­¢çœ‹åˆ°æœªæ¥token |
| **KV-Cache** | å¢é‡æ¨ç†åŠ é€Ÿ |

### å‚æ•°è§„æ¨¡å¯¹æ¯”

```mermaid
graph LR
    A[MiniMind Small<br/>26M] -->|4x| B[MiniMind Medium<br/>108M]
    B -->|1.3x| C[MiniMind MoE<br/>145M]
    C -->|69x| D[GPT-2<br/>10B]
    D -->|175x| E[GPT-3<br/>175B]
    
    style A fill:#90EE90
    style B fill:#FFD700
    style C fill:#FF6347
```

**å‚æ•°é‡å¯¹æ¯”**:
- **MiniMind Small**: 26M (æœ¬æ–‡æ¡£é‡ç‚¹)
- **MiniMind Medium**: 108M (4å€å‚æ•°)
- **MiniMind MoE**: 145M (4ä¸“å®¶,ä½†æ¿€æ´»å‚æ•°å°‘)
- **GPT-2**: 10B (385å€)
- **GPT-3**: 175B (6,730å€)

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph Input["è¾“å…¥å±‚"]
        TokenIds[Token IDs - batch, seq_len]
        TokenEmb[Token Embedding - ç»´åº¦: 512]
    end
    
    subgraph Transformer["Transformer å±‚ Ã— 8"]
        Layer1[Layer 1]
        Layer2[Layer 2]
        LayerN[Layer 8]
        
        Layer1 --> Layer2
        Layer2 -.-> LayerN
    end
    
    subgraph Output["è¾“å‡ºå±‚"]
        FinalNorm[Final LayerNorm]
        LMHead[LM Head - Linear: 512 to 6400]
        Logits[Logits - batch, seq_len, vocab]
    end
    
    TokenIds --> TokenEmb
    TokenEmb --> Layer1
    LayerN --> FinalNorm
    FinalNorm --> LMHead
    LMHead --> Logits
```

### æ¨¡å‹é…ç½®å‚æ•°

#### Small æ¨¡å‹é…ç½® (26M å‚æ•°)

| å‚æ•°å | å€¼ | è¯´æ˜ |
|--------|----|----- |
| `vocabSize` | 6,400 | è¯æ±‡è¡¨å¤§å° |
| `maxSeqLen` | 512 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `hiddenSize` | 512 | éšè—å±‚ç»´åº¦ |
| `numLayers` | 8 | Transformerå±‚æ•° |
| `numHeads` | 16 | æ³¨æ„åŠ›å¤´æ•° |
| `headDim` | 32 | æ¯ä¸ªå¤´çš„ç»´åº¦ (512/16) |
| `ffnHiddenSize` | 1,024 | FFNä¸­é—´å±‚ç»´åº¦ (2Ã—hiddenSize) |
| `dropout` | 0.1 | Dropoutæ¯”ä¾‹ |
| `activationFunction` | SiLU | æ¿€æ´»å‡½æ•° |
| `useRoPE` | true | ä½¿ç”¨RoPEä½ç½®ç¼–ç  |
| `preLayerNorm` | true | ä½¿ç”¨Pre-LNç»“æ„ |
| `epsilon` | 1e-5 | LayerNorm epsilon |

#### å‚æ•°é‡ä¼°ç®—

```java
// Token Embedding
vocabSize Ã— hiddenSize = 6,400 Ã— 512 = 3.28M

// Transformer Layer (Ã—8)
per_layer = {
    // LayerNorm1: 2 Ã— hiddenSize = 1,024
    // Multi-Head Attention
    W_qkv = 3 Ã— hiddenSize Ã— hiddenSize = 786,432
    W_o = hiddenSize Ã— hiddenSize = 262,144
    // LayerNorm2: 2 Ã— hiddenSize = 1,024
    // FFN
    W1 = hiddenSize Ã— ffnHiddenSize = 524,288
    W2 = ffnHiddenSize Ã— hiddenSize = 524,288
    
    Total per layer â‰ˆ 2.10M
}
8 layers = 16.80M

// Final LayerNorm + LM Head
finalNorm = 1,024
lmHead = hiddenSize Ã— vocabSize = 3.28M

// æ€»å‚æ•°é‡
Total â‰ˆ 3.28M + 16.80M + 3.28M = 23.36M â‰ˆ 26M
```

---

## ğŸ” Transformer å±‚è¯¦è§£

### å±‚ç»“æ„

æ¯ä¸ª Transformer å±‚åŒ…å«:

```mermaid
graph TB
    Input[è¾“å…¥ x<br/>[batch, seq, hidden]]
    
    subgraph Attention["æ³¨æ„åŠ›å­å±‚"]
        Norm1[LayerNorm 1]
        MHA[Multi-Head Attention]
        Add1[æ®‹å·®è¿æ¥ +]
    end
    
    subgraph FFN_Block["FFN å­å±‚"]
        Norm2[LayerNorm 2]
        FFN[Feed-Forward Network]
        Add2[æ®‹å·®è¿æ¥ +]
    end
    
    Output[è¾“å‡º<br/>[batch, seq, hidden]]
    
    Input --> Norm1
    Norm1 --> MHA
    MHA --> Add1
    Input --> Add1
    
    Add1 --> Norm2
    Norm2 --> FFN
    FFN --> Add2
    Add1 --> Add2
    
    Add2 --> Output
    
    style Attention fill:#e1f5ff
    style FFN_Block fill:#ffe1e1
```

### è®¡ç®—å…¬å¼

**Pre-LayerNorm ç»“æ„**:

```
# æ³¨æ„åŠ›å­å±‚
x1 = LayerNorm(x)
x2 = MultiHeadAttention(x1) + x  # æ®‹å·®è¿æ¥

# FFN å­å±‚
x3 = LayerNorm(x2)
x4 = FFN(x3) + x2  # æ®‹å·®è¿æ¥

output = x4
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ Pre-LN**:
- âœ… è®­ç»ƒæ›´ç¨³å®š,æ¢¯åº¦æ›´å¹³æ»‘
- âœ… æ”¶æ•›é€Ÿåº¦æ›´å¿«
- âœ… ä¸éœ€è¦å­¦ä¹ ç‡ warmup (å¯é€‰)

---

## ğŸ¯ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

### æ³¨æ„åŠ›è®¡ç®—æµç¨‹

```mermaid
graph TB
    X[è¾“å…¥ X - batch, seq, 512]
    
    subgraph Projection["çº¿æ€§æŠ•å½±"]
        Q[Q = XÂ·W_q - batch, seq, 512]
        K[K = XÂ·W_k - batch, seq, 512]
        V[V = XÂ·W_v - batch, seq, 512]
    end
    
    subgraph RoPE["RoPE ä½ç½®ç¼–ç "]
        Q_rope[Q_rope æ—‹è½¬ä½ç½®ç¼–ç ]
        K_rope[K_rope æ—‹è½¬ä½ç½®ç¼–ç ]
    end
    
    subgraph MultiHead["å¤šå¤´åˆ†å‰²"]
        Q_split[Q: batch, 16, seq, 32]
        K_split[K: batch, 16, seq, 32]
        V_split[V: batch, 16, seq, 32]
    end
    
    subgraph Attention["Scaled Dot-Product"]
        Score[Score = QÂ·KT / sqrt32]
        Mask[Causal Mask]
        Softmax[Softmax]
        Attn[AttnÂ·V]
    end
    
    subgraph Output["è¾“å‡ºæŠ•å½±"]
        Concat[Concat Heads - batch, seq, 512]
        O[O = ConcatÂ·W_o]
    end
    
    X --> Q
    X --> K
    X --> V
    
    Q --> Q_rope
    K --> K_rope
    V --> V_split
    
    Q_rope --> Q_split
    K_rope --> K_split
    
    Q_split --> Score
    K_split --> Score
    Score --> Mask
    Mask --> Softmax
    Softmax --> Attn
    V_split --> Attn
    
    Attn --> Concat
    Concat --> O
```

### è¯¦ç»†å‚æ•°

**æŠ•å½±çŸ©é˜µ**:
- `W_q`: [512, 512] - Query æŠ•å½±
- `W_k`: [512, 512] - Key æŠ•å½±
- `W_v`: [512, 512] - Value æŠ•å½±
- `W_o`: [512, 512] - è¾“å‡ºæŠ•å½±

**å¤šå¤´åˆ’åˆ†**:
- å¤´æ•°: 16
- æ¯ä¸ªå¤´ç»´åº¦: 512 / 16 = 32
- å¹¶è¡Œè®¡ç®— 16 ä¸ªå­ç©ºé—´

**Scaled Dot-Product Attention**:
```python
score = (Q @ K^T) / sqrt(head_dim)  # [batch, heads, seq, seq]
score = score + causal_mask  # å±è”½æœªæ¥ä½ç½®
attn_weights = softmax(score, dim=-1)  # å½’ä¸€åŒ–
output = attn_weights @ V  # [batch, heads, seq, head_dim]
```

### å› æœæ©ç  (Causal Mask)

**ä½œç”¨**: ç¡®ä¿ä½ç½® `i` åªèƒ½çœ‹åˆ°ä½ç½® `â‰¤ i` çš„ token,å®ç°è‡ªå›å½’ç”Ÿæˆã€‚

**æ©ç çŸ©é˜µç¤ºä¾‹** (seq_len=4):
```
[[0,  -inf, -inf, -inf],
 [0,   0,   -inf, -inf],
 [0,   0,    0,   -inf],
 [0,   0,    0,    0  ]]
```

**å®ç°**:
```java
// åˆ›å»ºå› æœæ©ç 
float[][] mask = new float[seqLen][seqLen];
for (int i = 0; i < seqLen; i++) {
    for (int j = i + 1; j < seqLen; j++) {
        mask[i][j] = Float.NEGATIVE_INFINITY;
    }
}
```

---

## ğŸŒ€ RoPE ä½ç½®ç¼–ç 

### åŸç†

RoPE (Rotary Position Embedding) é€šè¿‡æ—‹è½¬çŸ©é˜µå¯¹ Qã€K å‘é‡è¿›è¡Œä½ç½®ç¼–ç ,**ç›¸å¯¹ä½ç½®**é€šè¿‡æ—‹è½¬è§’åº¦å·®å¼‚ä½“ç°ã€‚

**ä¼˜åŠ¿**:
- âœ… ç›¸å¯¹ä½ç½®å»ºæ¨¡æ›´è‡ªç„¶
- âœ… æ”¯æŒé•¿åº¦å¤–æ¨ (å¦‚ YaRN)
- âœ… ä¸å¢åŠ å‚æ•°é‡
- âœ… è®¡ç®—é«˜æ•ˆ

### ç¼–ç å…¬å¼

å¯¹äºä½ç½® `m` å’Œç»´åº¦ `i`:

```
Î¸_i = 10000^(-2i/d)
RoPE(x_m, i) = x_m Â· cos(mÂ·Î¸_i) - x_{m+1} Â· sin(mÂ·Î¸_i)
RoPE(x_{m+1}, i) = x_m Â· sin(mÂ·Î¸_i) + x_{m+1} Â· cos(mÂ·Î¸_i)
```

**ä¼ªä»£ç **:
```python
def apply_rope(q, k, positions):
    # positions: [batch, seq_len]
    # q, k: [batch, heads, seq, head_dim]
    
    # è®¡ç®—é¢‘ç‡
    freqs = 1.0 / (10000 ** (torch.arange(0, head_dim, 2) / head_dim))
    
    # è®¡ç®—è§’åº¦
    angles = positions[:, :, None] * freqs[None, None, :]  # [batch, seq, head_dim/2]
    
    # æ—‹è½¬å˜æ¢
    q_rope = rotate_half(q, angles)
    k_rope = rotate_half(k, angles)
    
    return q_rope, k_rope
```

### å®ç°è¦ç‚¹

1. **ä»…å¯¹ Qã€K åº”ç”¨**: V ä¸éœ€è¦ä½ç½®ä¿¡æ¯
2. **æˆå¯¹æ—‹è½¬**: æ¯ä¸¤ä¸ªç»´åº¦ä¸€ç»„è¿›è¡Œæ—‹è½¬
3. **ç›¸å¯¹ä½ç½®**: `Q[m]Â·K[n]` çš„å€¼å–å†³äº `m-n`

---

## ğŸ” Feed-Forward Network (FFN)

### ç»“æ„

**ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ**:
```
FFN(x) = W2 Â· SiLU(W1 Â· x + b1) + b2
```

**ç»´åº¦å˜åŒ–**:
```
[batch, seq, 512] 
  â†’ Linear(512, 1024) 
  â†’ SiLU 
  â†’ Linear(1024, 512) 
  â†’ [batch, seq, 512]
```

### SiLU æ¿€æ´»å‡½æ•°

**å…¬å¼**:
```
SiLU(x) = x Â· Ïƒ(x) = x / (1 + e^(-x))
```

**å¯¹æ¯”å…¶ä»–æ¿€æ´»å‡½æ•°**:

| æ¿€æ´»å‡½æ•° | å…¬å¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|---------|------|------|------|
| **ReLU** | max(0, x) | ç®€å•,å¿«é€Ÿ | æ¢¯åº¦æ¶ˆå¤±,è´Ÿå€¼ä¿¡æ¯ä¸¢å¤± |
| **GELU** | xÂ·Î¦(x) | å¹³æ»‘,æ€§èƒ½å¥½ | è®¡ç®—ç¨æ…¢ |
| **SiLU** | xÂ·Ïƒ(x) | å¹³æ»‘,æ€§èƒ½ä¼˜ç§€ | è®¡ç®—æ¯”ReLUæ…¢ |

**ä¸ºä»€ä¹ˆé€‰æ‹© SiLU**:
- âœ… å¹³æ»‘å¯å¾®,æ¢¯åº¦æ›´ç¨³å®š
- âœ… åœ¨ Transformer ä¸­è¡¨ç°ä¼˜äº ReLU
- âœ… ä¸ GELU æ€§èƒ½ç›¸å½“,ä½†è®¡ç®—æ›´ç®€å•

### å‚æ•°é‡

```
W1: [512, 1024] = 524,288
b1: [1024] = 1,024
W2: [1024, 512] = 524,288
b2: [512] = 512

Total = 1,050,112 â‰ˆ 1.05M per layer
```

---

## ğŸš€ KV-Cache å¢é‡æ¨ç†

### é—®é¢˜èƒŒæ™¯

**è‡ªå›å½’ç”Ÿæˆ**: é€ä¸ªç”Ÿæˆ token,æ¯æ¬¡éœ€è¦å‰å‘ä¼ æ’­æ•´ä¸ªåºåˆ—ã€‚

**ä½æ•ˆç¤ºä¾‹**:
```
ç”Ÿæˆ "Hello world!"
Step 1: [Hello] â†’ predict " "
Step 2: [Hello, ] â†’ predict "world"
Step 3: [Hello, world] â†’ predict "!"

æ¯æ­¥éƒ½é‡æ–°è®¡ç®— [Hello] çš„ Kã€V,é€ æˆé‡å¤è®¡ç®—!
```

### KV-Cache åŸç†

**æ ¸å¿ƒæ€æƒ³**: ç¼“å­˜å·²è®¡ç®—çš„ Kã€V,åªè®¡ç®—æ–° token çš„ Kã€Vã€‚

```mermaid
graph LR
    subgraph Step1["Step 1: Hello"]
        Q1[Q: Hello]
        K1[K: Hello]
        V1[V: Hello]
        Cache1[(Cache<br/>K1, V1)]
    end
    
    subgraph Step2["Step 2: world"]
        Q2[Q: world]
        K2[K: world]
        V2[V: world]
        Cache2[(Cache<br/>K1,K2<br/>V1,V2)]
    end
    
    Q1 --> Attn1[Attn1]
    K1 --> Attn1
    V1 --> Attn1
    Attn1 --> Cache1
    
    Cache1 --> Attn2[Attn2]
    Q2 --> Attn2
    K2 --> Cache2
    V2 --> Cache2
    
    style Cache1 fill:#90EE90
    style Cache2 fill:#90EE90
```

### åŠ é€Ÿæ•ˆæœ

**æ—¶é—´å¤æ‚åº¦å¯¹æ¯”**:

| æ–¹æ³• | æ¯æ­¥è®¡ç®—é‡ | æ€»è®¡ç®—é‡ (ç”Ÿæˆnä¸ªtoken) |
|------|-----------|----------------------|
| **æ—  Cache** | O(LÂ²) | O(nÂ·LÂ²) |
| **KV-Cache** | O(L) | O(nÂ·L) |

å…¶ä¸­ `L` æ˜¯åºåˆ—é•¿åº¦ã€‚

**å®æµ‹åŠ é€Ÿ** (ç”Ÿæˆ100ä¸ªtoken):
- æ—  Cache: ~10ç§’
- æœ‰ Cache: ~1ç§’
- **åŠ é€Ÿæ¯”**: 10Ã—

### å®ç°ç¤ºä¾‹

```java
public class KVCacheManager {
    // ç¼“å­˜ç»“æ„: [layer_idx][batch][heads][cached_seq_len][head_dim]
    private List<NdArray> keyCaches;    // Kç¼“å­˜
    private List<NdArray> valueCaches;  // Vç¼“å­˜
    
    public void update(int layerIdx, NdArray newK, NdArray newV) {
        if (keyCaches.get(layerIdx) == null) {
            // é¦–æ¬¡ç”Ÿæˆ,ç›´æ¥å­˜å‚¨
            keyCaches.set(layerIdx, newK);
            valueCaches.set(layerIdx, newV);
        } else {
            // åç»­ç”Ÿæˆ,æ‹¼æ¥åˆ°ç¼“å­˜
            keyCaches.set(layerIdx, concat(keyCaches.get(layerIdx), newK, axis=2));
            valueCaches.set(layerIdx, concat(valueCaches.get(layerIdx), newV, axis=2));
        }
    }
    
    public Pair<NdArray, NdArray> get(int layerIdx) {
        return Pair.of(keyCaches.get(layerIdx), valueCaches.get(layerIdx));
    }
}
```

---

## ğŸ² é‡‡æ ·ç­–ç•¥

### é‡‡æ ·ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | Temperature | Top-K | Top-P | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|-------------|-------|-------|------|----------|
| **è´ªå©ªé‡‡æ ·** | 0.0 | - | - | ç¡®å®šæ€§,æ€»é€‰æœ€å¤§æ¦‚ç‡ | äº‹å®æ€§å›ç­” |
| **æ¸©åº¦é‡‡æ ·** | 0.1-2.0 | - | - | æ§åˆ¶éšæœºæ€§ | é€šç”¨ç”Ÿæˆ |
| **Top-K** | 1.0 | 20-100 | - | ä»…ä»å‰Kä¸ªå€™é€‰ä¸­é‡‡æ · | æ§åˆ¶å¤šæ ·æ€§ |
| **Top-P** | 1.0 | - | 0.8-0.95 | åŠ¨æ€é€‰æ‹©,ç´¯ç§¯æ¦‚ç‡è¾¾P | å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ |

### è´ªå©ªé‡‡æ · (Greedy Sampling)

**ç­–ç•¥**: å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenã€‚

```java
int nextToken = argmax(logits);  // é€‰æ‹©æœ€å¤§æ¦‚ç‡
```

**ä¼˜ç‚¹**:
- âœ… ç¡®å®šæ€§,ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º
- âœ… è¾“å‡ºè´¨é‡ç¨³å®š

**ç¼ºç‚¹**:
- âŒ ç¼ºä¹å¤šæ ·æ€§
- âŒ å®¹æ˜“é‡å¤

**é€‚ç”¨**: ç¿»è¯‘ã€äº‹å®é—®ç­”ç­‰éœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡ã€‚

### æ¸©åº¦é‡‡æ · (Temperature Sampling)

**ç­–ç•¥**: é€šè¿‡æ¸©åº¦å‚æ•°è°ƒèŠ‚æ¦‚ç‡åˆ†å¸ƒçš„"é”åˆ©"ç¨‹åº¦ã€‚

```java
// åº”ç”¨æ¸©åº¦
for (int i = 0; i < logits.length; i++) {
    logits[i] /= temperature;
}
// Softmax å¾—åˆ°æ¦‚ç‡
float[] probs = softmax(logits);
// ä»åˆ†å¸ƒä¸­é‡‡æ ·
int nextToken = sampleFromDistribution(probs);
```

**æ¸©åº¦æ•ˆæœ**:
- `temperature = 0.1`: æ¥è¿‘è´ªå©ª,ç¡®å®šæ€§å¼º
- `temperature = 1.0`: æ ‡å‡†é‡‡æ ·
- `temperature = 2.0`: æ›´éšæœº,å¤šæ ·æ€§é«˜

**ç¤ºä¾‹**:
```
åŸå§‹ logits: [2.0, 1.5, 1.0]

temperature = 0.5:
  logits â†’ [4.0, 3.0, 2.0]
  probs â†’ [0.67, 0.24, 0.09]  # é«˜æ¦‚ç‡æ›´é«˜

temperature = 2.0:
  logits â†’ [1.0, 0.75, 0.5]
  probs â†’ [0.42, 0.32, 0.26]  # æ¦‚ç‡æ›´å¹³å‡
```

### Top-K é‡‡æ ·

**ç­–ç•¥**: ä»…ä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ª token ä¸­é‡‡æ ·ã€‚

```java
// 1. å¯¹ logits æ’åº
int[] topK = selectTopK(logits, k);  // é€‰å‡ºå‰Kä¸ªç´¢å¼•

// 2. ä»…ä¿ç•™ Top-K,å…¶ä»–è®¾ä¸º -inf
for (int i = 0; i < logits.length; i++) {
    if (!topK.contains(i)) {
        logits[i] = Float.NEGATIVE_INFINITY;
    }
}

// 3. Softmax å¹¶é‡‡æ ·
float[] probs = softmax(logits);
int nextToken = sampleFromDistribution(probs);
```

**K å€¼å½±å“**:
- `K = 1`: ç­‰ä»·äºè´ªå©ªé‡‡æ ·
- `K = 10`: éå¸¸ä¿å®ˆ,è´¨é‡é«˜
- `K = 50`: ä¸­ç­‰å¤šæ ·æ€§
- `K = 100`: é«˜å¤šæ ·æ€§,å¯èƒ½æœ‰ä½è´¨é‡ token

### Top-P é‡‡æ · (Nucleus Sampling)

**ç­–ç•¥**: åŠ¨æ€é€‰æ‹©ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„æœ€å° token é›†åˆã€‚

```java
// 1. å¯¹æ¦‚ç‡æ’åº
float[] probs = softmax(logits);
int[] sortedIndices = argsort(probs, descending=true);

// 2. ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P
float cumProb = 0.0f;
List<Integer> nucleus = new ArrayList<>();
for (int idx : sortedIndices) {
    cumProb += probs[idx];
    nucleus.add(idx);
    if (cumProb >= topP) break;
}

// 3. ä» nucleus ä¸­é‡‡æ ·
int nextToken = sampleFromNucleus(nucleus, probs);
```

**P å€¼å½±å“**:
- `P = 0.9`: ä¿å®ˆ,é«˜è´¨é‡
- `P = 0.95`: å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§
- `P = 0.99`: é«˜å¤šæ ·æ€§

**ä¼˜åŠ¿**: è‡ªé€‚åº”å€™é€‰é›†å¤§å°,é¿å… Top-K çš„å›ºå®š K å€¼é™åˆ¶ã€‚

### æ¨èé…ç½®

| ä»»åŠ¡ | Temperature | Top-K | Top-P | è¯´æ˜ |
|------|-------------|-------|-------|------|
| **ä»£ç ç”Ÿæˆ** | 0.2 | 0 | 0.95 | å‡†ç¡®æ€§ä¼˜å…ˆ |
| **å¯¹è¯èŠå¤©** | 0.7 | 0 | 0.9 | å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ |
| **åˆ›æ„å†™ä½œ** | 0.9 | 0 | 0.95 | å¤šæ ·æ€§ä¼˜å…ˆ |
| **ç¿»è¯‘** | 0.0 | 0 | 0.0 | è´ªå©ªé‡‡æ · |

---

## ğŸ‹ï¸ è®­ç»ƒç­–ç•¥

### è®­ç»ƒé˜¶æ®µæ¦‚è§ˆ

```mermaid
graph LR
    Pretrain[é¢„è®­ç»ƒ<br/>æ— æ ‡æ³¨æ–‡æœ¬] --> SFT[ç›‘ç£å¾®è°ƒ<br/>æŒ‡ä»¤æ•°æ®]
    SFT --> LoRA[LoRAå¾®è°ƒ<br/>é«˜æ•ˆå¾®è°ƒ]
    LoRA --> DPO[DPOè®­ç»ƒ<br/>åå¥½å¯¹é½]
    DPO --> Deploy[éƒ¨ç½²åº”ç”¨]
    
    style Pretrain fill:#e1f5ff
    style SFT fill:#fff4e1
    style LoRA fill:#ffe1e1
    style DPO fill:#e1ffe1
```

### 1. é¢„è®­ç»ƒ (Pretraining)

**ç›®æ ‡**: å­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹å’ŒçŸ¥è¯†ã€‚

**æ•°æ®**: å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ (å¦‚ç»´åŸºç™¾ç§‘ã€ä¹¦ç±ã€ç½‘é¡µ)ã€‚

**æŸå¤±å‡½æ•°**: äº¤å‰ç†µæŸå¤±
```
Loss = -âˆ‘ log P(token_i | token_<i)
```

**å…³é”®é…ç½®**:
```java
PretrainConfig config = new PretrainConfig();
config.setLearningRate(3e-4f);      // è¾ƒå¤§å­¦ä¹ ç‡
config.setBatchSize(32);
config.setNumEpochs(10);
config.setWarmupSteps(2000);        // Warmup é˜²æ­¢è®­ç»ƒåˆæœŸéœ‡è¡
config.setMaxGradNorm(1.0f);        // æ¢¯åº¦è£å‰ª
```

**è®­ç»ƒæ—¶é—´**: å•å¡ GPU,~2 å°æ—¶ã€‚

### 2. ç›‘ç£å¾®è°ƒ (SFT)

**ç›®æ ‡**: å­¦ä¹ éµå¾ªæŒ‡ä»¤,ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„å›ç­”ã€‚

**æ•°æ®æ ¼å¼** (JSONL):
```json
{"instruction": "å†™ä¸€é¦–è¯—", "input": "", "output": "æ˜¥çœ ä¸è§‰æ™“..."}
{"instruction": "ç¿»è¯‘", "input": "Hello", "output": "ä½ å¥½"}
```

**å…³é”®é…ç½®**:
```java
SFTConfig sftConfig = new SFTConfig();
sftConfig.setLearningRate(5e-5f);       // è¾ƒå°å­¦ä¹ ç‡
sftConfig.setLossOnOutputOnly(true);    // ä»…è®¡ç®—è¾“å‡ºéƒ¨åˆ†æŸå¤±
sftConfig.setNumEpochs(3);
```

**`lossOnOutputOnly` è§£é‡Š**:
- ä»…å¯¹ `output` éƒ¨åˆ†è®¡ç®—æŸå¤±,å¿½ç•¥ `instruction` å’Œ `input`
- é¿å…æ¨¡å‹å­¦ä¹ é‡å¤è¾“å…¥çš„è¡Œä¸º
- æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›

### 3. LoRA å¾®è°ƒ

**ç›®æ ‡**: é«˜æ•ˆå¾®è°ƒ,å‡å°‘å¯è®­ç»ƒå‚æ•°é‡ã€‚

**åŸç†**: åœ¨é¢„è®­ç»ƒæƒé‡ä¸Šæ·»åŠ ä½ç§©çŸ©é˜µ:
```
W' = W + Î”W = W + BÂ·A
```
å…¶ä¸­ `B: [d, r]`, `A: [r, d]`, `r << d` (å¦‚ r=8)ã€‚

**å‚æ•°é‡å¯¹æ¯”**:
- å…¨å‚æ•°å¾®è°ƒ: 26M
- LoRA å¾®è°ƒ (r=8): ~0.5M (ä»… 1.9%)

**å…³é”®é…ç½®**:
```java
LoRAConfig loraConfig = new LoRAConfig();
loraConfig.setRank(8);                          // ç§© r
loraConfig.setAlpha(16.0f);                     // ç¼©æ”¾å› å­ Î±
loraConfig.setTargetModules(Arrays.asList("q_proj", "v_proj"));
```

**ç¼©æ”¾**: 
```
Î”W_scaled = (Î± / r) Â· BÂ·A
```

### 4. DPO è®­ç»ƒ

**ç›®æ ‡**: æ ¹æ®äººç±»åå¥½å¯¹é½æ¨¡å‹è¾“å‡ºã€‚

**æ•°æ®æ ¼å¼**:
```json
{
  "prompt": "å†™ä¸€é¦–è¯—",
  "chosen": "æ˜¥çœ ä¸è§‰æ™“,å¤„å¤„é—»å•¼é¸Ÿ...",
  "rejected": "åºŠå‰æ˜æœˆå…‰,ç–‘æ˜¯åœ°ä¸Šéœœ..."
}
```

**æŸå¤±å‡½æ•°**:
```
L_DPO = -E[log Ïƒ(Î² Â· (log Ï€(y_w|x) - log Ï€(y_l|x) 
                        - log Ï€_ref(y_w|x) + log Ï€_ref(y_l|x)))]
```

å…¶ä¸­:
- `y_w`: chosen (ä¼˜é€‰)
- `y_l`: rejected (æ‹’ç»)
- `Ï€`: å½“å‰ç­–ç•¥
- `Ï€_ref`: å‚è€ƒæ¨¡å‹ (å†»ç»“)
- `Î²`: KL æƒ©ç½šç³»æ•°

**å…³é”®é…ç½®**:
```java
DPOConfig dpoConfig = new DPOConfig();
dpoConfig.setBeta(0.1f);            // KL æƒ©ç½šç³»æ•°
dpoConfig.setLearningRate(5e-6f);   // æå°å­¦ä¹ ç‡
```

---

## ğŸ§© MoE æ¶æ„æ‰©å±•

### MoE æ¦‚å¿µ

**Mixture-of-Experts (MoE)**: ç”¨å¤šä¸ª"ä¸“å®¶"ç½‘ç»œæ›¿ä»£å•ä¸€ FFN,æ¯æ¬¡æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ã€‚

**ä¼˜åŠ¿**:
- âœ… æ€»å‚æ•°é‡å¤§,ä½†æ¿€æ´»å‚æ•°å°‘
- âœ… æ¨ç†é€Ÿåº¦æ¥è¿‘å°æ¨¡å‹
- âœ… æ€§èƒ½æ¥è¿‘å¤§æ¨¡å‹

### MoE ç»“æ„

```mermaid
graph TB
    Input[è¾“å…¥ x]
    
    subgraph Router["è·¯ç”±ç½‘ç»œ"]
        Gate[é—¨æ§ç½‘ç»œ softmax W_gate x]
        TopK[Top-2 é€‰æ‹©]
    end
    
    subgraph Experts["ä¸“å®¶ç½‘ç»œ 4ä¸ª"]
        E1[Expert 1 FFN]
        E2[Expert 2 FFN]
        E3[Expert 3 FFN]
        E4[Expert 4 FFN]
    end
    
    subgraph Combine["åŠ æƒç»„åˆ"]
        Weight[æƒé‡ w1, w2]
        Sum[åŠ æƒæ±‚å’Œ]
    end
    
    Input --> Gate
    Gate --> TopK
    
    TopK -.-> E1
    TopK -.-> E3
    
    E1 --> Weight
    E3 --> Weight
    Weight --> Sum
    
    Sum --> Output[è¾“å‡º]
```

### MoE é…ç½®

**MiniMind MoE å‚æ•°**:
```java
MiniMindConfig config = MiniMindConfig.createMoEConfig();
config.setUseMoE(true);
config.setNumExperts(4);            // 4ä¸ªä¸“å®¶
config.setNumExpertsPerToken(2);    // æ¯æ¬¡æ¿€æ´»2ä¸ª
```

**å‚æ•°é‡å¯¹æ¯”**:
- Small (æ ‡å‡† FFN): 26M
- MoE (4ä¸“å®¶): 145M æ€»å‚æ•°
- MoE æ¿€æ´»å‚æ•°: ~72M (çº¦ 50%)

### è´Ÿè½½å‡è¡¡æŸå¤±

**é—®é¢˜**: è·¯ç”±å¯èƒ½æ€»é€‰æ‹©å°‘æ•°å‡ ä¸ªä¸“å®¶,å¯¼è‡´è´Ÿè½½ä¸å‡è¡¡ã€‚

**è§£å†³**: æ·»åŠ è´Ÿè½½å‡è¡¡æŸå¤±:
```
L_balance = Î± Â· âˆ‘_i f_i Â· P_i
```
å…¶ä¸­:
- `f_i`: ä¸“å®¶ i è¢«é€‰ä¸­çš„é¢‘ç‡
- `P_i`: ä¸“å®¶ i çš„å¹³å‡æ¦‚ç‡
- `Î±`: å¹³è¡¡ç³»æ•° (å¦‚ 0.01)

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ¨ç†ä¼˜åŒ–æŠ€å·§

| ä¼˜åŒ–æŠ€æœ¯ | åŠ é€Ÿæ¯” | é€‚ç”¨åœºæ™¯ |
|---------|--------|---------|
| **KV-Cache** | 5-10Ã— | è‡ªå›å½’ç”Ÿæˆ |
| **æ‰¹é‡æ¨ç†** | 2-5Ã— | å¤šè¯·æ±‚å¹¶å‘ |
| **é‡åŒ– (INT8)** | 2-4Ã— | å†…å­˜å—é™ç¯å¢ƒ |
| **Flash Attention** | 1.5-2Ã— | é•¿åºåˆ— |

### å†…å­˜ä¼˜åŒ–

**æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)**:
- è®­ç»ƒæ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»,åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
- å†…å­˜å ç”¨: â†“ 50-70%
- è®­ç»ƒé€Ÿåº¦: â†“ 20-30%

**æ··åˆç²¾åº¦è®­ç»ƒ (FP16)**:
- ä½¿ç”¨ FP16 å­˜å‚¨å’Œè®¡ç®—,FP32 ç´¯ç§¯æ¢¯åº¦
- å†…å­˜å ç”¨: â†“ 50%
- è®­ç»ƒé€Ÿåº¦: â†‘ 2-3Ã—

### è®­ç»ƒåŠ é€Ÿ

**æ•°æ®å¹¶è¡Œ**: å¤š GPU å¹¶è¡Œè®­ç»ƒ
```
æœ‰æ•ˆæ‰¹æ¬¡ = batch_size Ã— num_gpus Ã— gradient_accumulation_steps
```

**æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
```java
config.setGradientAccumulationSteps(4);  // ç´¯ç§¯4æ­¥åæ›´æ–°
```

---

## ğŸ”¬ æ¨¡å‹è¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å…¬å¼ |
|------|------|------|
| **Perplexity** | å›°æƒ‘åº¦,è¶Šä½è¶Šå¥½ | `PPL = exp(Loss)` |
| **Accuracy** | ä¸‹ä¸€ä¸ªtokené¢„æµ‹å‡†ç¡®ç‡ | `æ­£ç¡®é¢„æµ‹æ•° / æ€»é¢„æµ‹æ•°` |
| **BLEU** | ç¿»è¯‘è´¨é‡è¯„ä¼° | n-gram åŒ¹é…åº¦ |
| **Rouge** | æ‘˜è¦è´¨é‡è¯„ä¼° | å¬å›ç‡æŒ‡æ ‡ |

### åŸºå‡†æµ‹è¯•

**æ¨èæµ‹è¯•é›†**:
- **é€šç”¨**: WikiText-103
- **å¯¹è¯**: MultiWOZ
- **ä»£ç **: HumanEval
- **ä¸­æ–‡**: CLUE

**ç¤ºä¾‹ä»£ç **:
```java
// è®¡ç®—å›°æƒ‘åº¦
float totalLoss = 0.0f;
int numBatches = 0;

for (Batch batch : validDataset) {
    Variable logits = model.predict(batch.input);
    float loss = crossEntropyLoss(logits, batch.target);
    totalLoss += loss;
    numBatches++;
}

float avgLoss = totalLoss / numBatches;
float perplexity = (float) Math.exp(avgLoss);

System.out.println("Perplexity: " + perplexity);
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### è®­ç»ƒå»ºè®®

1. **å­¦ä¹ ç‡è®¾ç½®**:
   - é¢„è®­ç»ƒ: 3e-4
   - SFT: 5e-5
   - LoRA: 1e-4
   - DPO: 5e-6

2. **Warmup ç­–ç•¥**:
   ```
   lr = base_lr Ã— min(step / warmup_steps, 1.0)
   ```

3. **æ¢¯åº¦è£å‰ª**:
   ```java
   config.setMaxGradNorm(1.0f);  // é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
   ```

4. **å­¦ä¹ ç‡è¡°å‡**:
   ```
   lr_decay = base_lr Ã— (1 - step / max_steps)^0.5
   ```

### æ¨ç†å»ºè®®

1. **é‡‡æ ·å‚æ•°æ¨è**:
   - å¯¹è¯: `temperature=0.7, top_p=0.9`
   - ä»£ç : `temperature=0.2, top_p=0.95`
   - åˆ›æ„: `temperature=0.9, top_p=0.95`

2. **æ‰¹é‡å¤§å°**: æ ¹æ®å†…å­˜è°ƒæ•´
   ```
   max_batch_size = GPU_memory / (model_size + seq_len Ã— hidden_size)
   ```

3. **KV-Cache ç®¡ç†**: é•¿å¯¹è¯æ—¶å®šæœŸæ¸…ç†æ—§ç¼“å­˜

---

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡

- **Attention Is All You Need** (Vaswani et al., 2017) - Transformer åŸå§‹è®ºæ–‡
- **RoFormer** (Su et al., 2021) - RoPE ä½ç½®ç¼–ç 
- **LoRA** (Hu et al., 2021) - ä½ç§©é€‚é…å™¨
- **DPO** (Rafailov et al., 2023) - ç›´æ¥åå¥½ä¼˜åŒ–
- **Switch Transformers** (Fedus et al., 2021) - MoE æ¶æ„

### ä»£ç å‚è€ƒ

- [MiniMind åŸç‰ˆ](https://github.com/jingyaogong/minimind) - Python å®ç°
- [TinyAI NNet V2](../../../tinyai-deeplearning/tinyai-deeplearning-nnet) - V2 ç»„ä»¶åº“
- [TinyAI ML](../../../tinyai-deeplearning/tinyai-deeplearning-ml) - è®­ç»ƒæ¡†æ¶

### ç›¸å…³æ–‡æ¡£

- [API å‚è€ƒæ–‡æ¡£](./APIå‚è€ƒ.md)
- [å¿«é€Ÿå¼€å§‹æŒ‡å—](./å¿«é€Ÿå¼€å§‹æŒ‡å—.md)
- [æ¨¡å—åˆ›å»ºæ–‡æ¡£](./module-creation.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-12-07  
**ç»´æŠ¤è€…**: TinyAI Team
